{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.12 64-bit",
   "display_name": "Python 3.6.12 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cab2a85f98424baa3ee9755c87a8333b57a3c02ba1de25e97492887dab4fe3fb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import correctingagent.world.world_generation as world_generation\n",
    "from correctingagent.util.colour_dict import colour_dict\n",
    "from correctingagent.util.colour_dict import fruit_dict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'fruit-4'\n",
    "data_path = Path('/home/yucheng/Desktop/project/correcting-agent/data')\n",
    "top_path = data_path / dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = top_path / f'{dataset_name}{num_datasets}'\n",
    "os.makedirs(dataset_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PosixPath('/home/yucheng/Desktop/project/correcting-agent/data/fruit-4/fruit-40')"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [('purple', np.array([0.732085  , 0.92597844, 0.99624985])), ('orange', np.array([0.08524697, 0.90982403, 0.82954605]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_dic = {}\n",
    "for i, (colour, hsv) in enumerate(test):\n",
    "    if colour == 'purple':\n",
    "        new = np.array([1, 2, 3, 4])\n",
    "        fruit_dic[f\"b{i}\"] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_object_dict = {f\"b{i}\": tuple(hsv) for i, (colour, hsv) in enumerate(test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'b0': (0.732085, 0.92597844, 0.99624985),\n",
       " 'b1': (0.08524697, 0.90982403, 0.82954605)}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "colour_object_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'b0': array([1, 2, 3, 4])}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "fruit_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = Path(\"/home/yucheng/Desktop/project/correcting-agent/data/tmp\")\n",
    "json_name = directory / f\"colours.json\"\n",
    "with open(json_name, 'w') as f:\n",
    "    json.dump(colour_object_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, alpha=1, beta=1, gamma=1, latent_n=1, groups={}, device=\"cpu\"):\n",
    "        super(VAE, self).__init__()\n",
    "        layers = []\n",
    "        self.latent_n = latent_n\n",
    "        self.groups = groups\n",
    "        self.groups_n = len(groups.keys())\n",
    "        self.device = device\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # IMAGE ENCODER\n",
    "        self.conv_channels = [32,32,64,64]\n",
    "        self.dense_channels = [1024, 32]\n",
    "        self.deconv_channels = [64, 64]\n",
    "\n",
    "        kernel_size=7\n",
    "        self.encoder_conv_0 = nn.Conv2d(3, self.conv_channels[0], kernel_size, padding=3, stride=2) # (32, 32)\n",
    "        kernel_size=5\n",
    "        self.encoder_conv_1 = nn.Conv2d(self.conv_channels[0], self.conv_channels[1], kernel_size, padding=2, stride=2) # (16, 16)\n",
    "        kernel_size=3\n",
    "        self.encoder_conv_2 = nn.Conv2d(self.conv_channels[1], self.conv_channels[2], kernel_size, padding=1, stride=2) # (8, 8)\n",
    "        self.encoder_conv_3 = nn.Conv2d(self.conv_channels[2], self.conv_channels[3], kernel_size, padding=1, stride=2) # (4, 4)\n",
    "        \n",
    "        self.encoder_dense_0 = nn.Linear(self.dense_channels[0], self.dense_channels[1])\n",
    "        self.encoder_mu = nn.Linear(self.dense_channels[1], self.latent_n)\n",
    "        self.encoder_ln_var = nn.Linear(self.dense_channels[1], self.latent_n)\n",
    "\n",
    "        # IMAGE DECONV DECODER\n",
    "        self.decoder_dense_0 = nn.Linear(self.latent_n, self.dense_channels[1])\n",
    "        self.decoder_dense_1 = nn.Linear(self.dense_channels[1], self.dense_channels[0])\n",
    "        self.decoder_conv_3 = nn.Conv2d(self.conv_channels[3], self.conv_channels[2], kernel_size, padding=1)\n",
    "        self.decoder_conv_2 = nn.Conv2d(self.conv_channels[2], self.conv_channels[1], kernel_size, padding=1)\n",
    "        self.decoder_conv_1 = nn.Conv2d(self.conv_channels[1], self.conv_channels[1], kernel_size, padding=1)\n",
    "        self.decoder_output_img = nn.Conv2d(self.conv_channels[1], 3, kernel_size, padding=1)\n",
    "\n",
    "        # CLASSIFIERS\n",
    "        self.classifiers = nn.ModuleList([nn.Linear(1, len(items)) for key, items in self.groups.items()])\n",
    "        \n",
    "        self.encoder = [self.encoder_conv_0,\n",
    "                        self.encoder_conv_1,\n",
    "                        self.encoder_conv_2,\n",
    "                        self.encoder_conv_3,\n",
    "                        self.encoder_dense_0,\n",
    "                        self.encoder_mu,\n",
    "                        self.encoder_ln_var]\n",
    "        \n",
    "        self.decoder = [self.decoder_dense_0,\n",
    "                        self.decoder_dense_1,\n",
    "                        self.decoder_conv_3,\n",
    "                        self.decoder_conv_2,\n",
    "                        self.decoder_conv_1,\n",
    "                        self.decoder_output_img]\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for i in range(len(self.encoder)):\n",
    "            self.encoder[i].weight.data.normal_(0, 0.01)\n",
    "            \n",
    "        for i in range(len(self.decoder)):\n",
    "            self.decoder[i].weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \n",
    "        conv_0_encoded = F.leaky_relu(self.encoder_conv_0(x))\n",
    "        conv_1_encoded = F.leaky_relu(self.encoder_conv_1(conv_0_encoded))\n",
    "        conv_2_encoded = F.leaky_relu(self.encoder_conv_2(conv_1_encoded))\n",
    "        conv_3_encoded = F.leaky_relu(self.encoder_conv_3(conv_2_encoded))\n",
    "\n",
    "        reshaped_encoded = torch.flatten(conv_3_encoded, start_dim=1)\n",
    "        dense_0_encoded = F.leaky_relu(self.encoder_dense_0(reshaped_encoded))\n",
    "        mu = self.encoder_mu(dense_0_encoded)\n",
    "        logvar = self.encoder_ln_var(dense_0_encoded)\n",
    "        \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        return z, mu, logvar\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \n",
    "        dense_0_decoded = self.decoder_dense_0(z)\n",
    "        dense_1_decoded = self.decoder_dense_1(dense_0_decoded)\n",
    "        reshaped_decoded = dense_1_decoded.view((len(dense_1_decoded), self.conv_channels[-1], 4, 4))\n",
    "        up_4_decoded = torch.nn.Upsample(scale_factor=2)(reshaped_decoded)\n",
    "        deconv_3_decoded = F.relu(self.decoder_conv_3(up_4_decoded))\n",
    "        up_3_decoded = torch.nn.Upsample(scale_factor=2)(deconv_3_decoded)\n",
    "        deconv_2_decoded = F.relu(self.decoder_conv_2(up_3_decoded))\n",
    "        up_2_decoded = torch.nn.Upsample(scale_factor=2)(deconv_2_decoded)\n",
    "        deconv_1_decoded = F.relu(self.decoder_conv_1(up_2_decoded))\n",
    "        up_1_decoded = torch.nn.Upsample(scale_factor=2)(deconv_1_decoded)\n",
    "        out_img = self.decoder_output_img(up_1_decoded)\n",
    "        \n",
    "        return torch.sigmoid(out_img)\n",
    "    \n",
    "    def predict_labels(self, z, softmax=False):\n",
    "        result = []\n",
    "        \n",
    "        for i in range(self.groups_n):\n",
    "            prediction = self.classifiers[i](z[:, i, None])\n",
    "\n",
    "            # need the check because the softmax_cross_entropy has a softmax in it\n",
    "            if softmax:\n",
    "                result.append(F.softmax(prediction))\n",
    "            else:\n",
    "                result.append(prediction)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def get_latent(self, x):\n",
    "        mu, logvar, _ = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        # img_out = self.sp_decode(z)\n",
    "        img_out = self.decode(z)\n",
    "        labels_out = self.predict_labels(z)\n",
    "        \n",
    "        return img_out, labels_out, mu, logvar \n",
    "    \n",
    "    def get_loss(self):\n",
    "        \n",
    "        def loss(img_in, img_out, labels_in, labels_out, mu, logvar):\n",
    "            \n",
    "            rec = nn.MSELoss(reduction=\"none\")(img_out, img_in)\n",
    "            rec = torch.mean(torch.sum(rec.view(rec.shape[0], -1), dim=-1))\n",
    "\n",
    "            label = 0\n",
    "            for i in range(self.groups_n):\n",
    "#                 print(i)\n",
    "#                 print('label out')\n",
    "#                 print(labels_out[i])\n",
    "#                 print('----')\n",
    "#                 print('label in')\n",
    "#                 print(labels_in)\n",
    "#                 print('----')\n",
    "#                 label += nn.CrossEntropyLoss(ignore_index=100)(labels_out[i], labels_in[:, i])\n",
    "                label += nn.CrossEntropyLoss(ignore_index=100)(labels_out[i], labels_in)\n",
    "                \n",
    "#                 print(label)\n",
    "            kld = (((-0.5) * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())) / img_in.shape[0])\n",
    "\n",
    "            rec *= self.alpha\n",
    "            kld *= self.beta\n",
    "            label *= self.gamma\n",
    "\n",
    "            return rec + label + kld, rec, label, kld\n",
    "    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fruit_loader(num=400):\n",
    "    train_path = '/home/yucheng/Desktop/project/weak_label_lfd/fruit/Train/*'\n",
    "    test_path = '/home/yucheng/Desktop/project/weak_label_lfd/fruit/test/*'\n",
    "    crop_size = 64\n",
    "    \n",
    "    training_fruit_img = []\n",
    "    training_label = []\n",
    "    test_fruit_img = []\n",
    "    test_label = []\n",
    "#     total_fruit_img = []\n",
    "#     total_label = []\n",
    "    \n",
    "    # load training images(#num per class)\n",
    "    for dir_path in glob.glob(train_path):\n",
    "        img_label = dir_path.split(\"/\")[-1]\n",
    "        count = 0\n",
    "        for image_path in glob.glob(os.path.join(dir_path,\"*.jpg\")):\n",
    "            image = cv2.imread(image_path,cv2.IMREAD_COLOR)\n",
    "            image = cv2.resize(image, (crop_size, crop_size))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            training_fruit_img.append(image)\n",
    "            training_label.append(img_label)\n",
    "            count += 1\n",
    "            if count == num:\n",
    "                break\n",
    "#             elif count < k+50:\n",
    "#                 image = cv2.imread(image_path,cv2.IMREAD_COLOR)\n",
    "#                 image = cv2.resize(image, (crop_size, crop_size))\n",
    "#                 image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#                 test_fruit_img.append(image)\n",
    "#                 test_label.append(img_label)\n",
    "#                 count = count + 1\n",
    "#             else:\n",
    "#                 break\n",
    "\n",
    "    # load all test images\n",
    "    for dir_path in glob.glob(test_path):\n",
    "        img_label = dir_path.split(\"/\")[-1]\n",
    "        count = 0\n",
    "        for image_path in glob.glob(os.path.join(dir_path,\"*.jpg\")):\n",
    "            image = cv2.imread(image_path,cv2.IMREAD_COLOR)\n",
    "            image = cv2.resize(image, (crop_size, crop_size))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            test_fruit_img.append(image)\n",
    "            test_label.append(img_label)\n",
    "            count += 1\n",
    "            if count == 100:\n",
    "                break \n",
    "            \n",
    "    train_imgs = np.array(training_fruit_img)\n",
    "    train_labels = np.array(training_label)\n",
    "    test_imgs = np.array(test_fruit_img)\n",
    "    test_labels = np.array(test_label)\n",
    "    \n",
    "    return train_imgs, train_labels, test_imgs, test_labels\n",
    "#     return total_imgs, total_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(k, train_imgs, train_labels, test_imgs, test_labels,batchsize=32, readall=True, num=400):\n",
    "    n_class = 10\n",
    "    number_of_training = len(train_imgs) #total number\n",
    "    number_of_test = len(test_imgs)\n",
    "    train_n = k*n_class # how many training sample we need \n",
    "    \n",
    "    if readall:\n",
    "        train_indecies = np.random.choice(range(number_of_training), train_n, replace=False) # choose k*n_class number of training samples\n",
    "        test_indecies = np.random.choice(range(number_of_test), 100, replace=False) # choose k number of test \n",
    "    else:\n",
    "        train_indecies = []\n",
    "        test_indecies = []\n",
    "    \n",
    "        for i in range(n_class):\n",
    "            class_indecies = np.random.choice(range(num), k, replace=False) # random choose k number of training samples per class\n",
    "            test_class_indecies = np.random.choice(range(100), 5, replace=False) # random pick 5 sample for each class\n",
    "            train_indecies.extend(class_indecies + num*i)\n",
    "            test_indecies.extend(test_class_indecies + 100*i)\n",
    "    \n",
    "    # print(train_indecies)\n",
    "\n",
    "    label_to_id = {v:k for k,v in enumerate(np.unique(train_labels)) }\n",
    "    id_to_label = {v:k for k,v in label_to_id.items() }\n",
    "\n",
    "    label_id = np.array([label_to_id[i] for i in train_labels])\n",
    "    one_hot_label = np.zeros((label_id.shape[0], n_class))\n",
    "    one_hot_label[np.arange(label_id.shape[0]), label_id] = 1\n",
    "\n",
    "    train_imgs = np.swapaxes(train_imgs, 1, 3) # (n, 3, 64, 64)\n",
    "    train_imgs = train_imgs/255\n",
    "    \n",
    "    \n",
    "#     label_to_id_t = {v:k for k,v in enumerate(np.unique(test_labels)) }\n",
    "#     id_to_label_t = {v:k for k,v in label_to_id_t.items() }\n",
    "\n",
    "    label_id_t = np.array([label_to_id[i] for i in test_labels])\n",
    "    one_hot_label_t = np.zeros((label_id_t.shape[0], n_class))\n",
    "    one_hot_label_t[np.arange(label_id_t.shape[0]), label_id_t] = 1\n",
    "\n",
    "    test_imgs = np.swapaxes(test_imgs, 1, 3) # (n, 3, 64, 64)\n",
    "    test_imgs = test_imgs/255\n",
    "    \n",
    "#     imgs = imgs.astype(np.float32)\n",
    "#     one_hot_label = one_hot_label.astype(np.long)\n",
    "    \n",
    "    train_images = np.take(train_imgs, train_indecies, axis=0).astype(np.float32)\n",
    "    train_labels = np.take(one_hot_label, train_indecies, axis=0).astype(np.long)\n",
    "    label_id = np.take(label_id, train_indecies, axis=0).astype(np.long)\n",
    "    # print(label_id)\n",
    "\n",
    "    test_images = np.take(test_imgs, test_indecies, axis=0).astype(np.float32)\n",
    "    test_labels = np.take(one_hot_label_t, test_indecies, axis=0).astype(np.long)\n",
    "    label_id_t = np.take(label_id_t, test_indecies, axis=0).astype(np.long)\n",
    "    #random shuffle training data\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(train_images)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(train_labels)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(label_id)\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_images), torch.tensor(label_id))\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batchsize, shuffle=False, num_workers=0)\n",
    "        \n",
    "    test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_images), torch.tensor(label_id_t))\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "    \n",
    "    \n",
    "    return train_dataloader, train_images, train_labels, label_id, test_dataloader, test_images, test_labels, label_id_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_100, train_labels_100, test_imgs_100, test_labels_100 = fruit_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 64, 64, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "test_imgs_100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 400\n",
    "if k == 5:\n",
    "    batchsize = 1\n",
    "else:\n",
    "    batchsize = 32\n",
    "num = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "groups = {}\n",
    "groups[0] = list(range(10))\n",
    "\n",
    "PATH = \"/home/yucheng/Desktop/project/correcting-agent/results_0128\"\n",
    "model_name = \"model_k{}_0\".format(k)\n",
    "latent_n = 4\n",
    "# net = VAE(latent_n=latent_n, groups=data_generator.groups)\n",
    "net = VAE(latent_n=latent_n, groups=groups)\n",
    "net.load_state_dict(torch.load(osp.join(PATH, model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, train_images, train_labels, label_id, testloader, test_images, test_labels, label_id_t= preprocessing(k, train_imgs_100, train_labels_100, test_imgs_100, test_labels_100, batchsize=batchsize, readall=False, num=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_images' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0ea52c174956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_id_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_images' is not defined"
     ]
    }
   ],
   "source": [
    "imgs = test_images\n",
    "labels = label_id_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[35, 36, 37, 38, 39]\n[30, 31, 32, 33, 34]\n[40, 41, 42, 43, 44]\n[15, 16, 17, 18, 19]\n[5, 6, 7, 8, 9]\n[45, 46, 47, 48, 49]\n[25, 26, 27, 28, 29]\n[20, 21, 22, 23, 24]\n[0, 1, 2, 3, 4]\n[10, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "latent = {}\n",
    "stats = {}\n",
    "for group_idx in groups.keys():\n",
    "    latent[group_idx] = {}\n",
    "    stats[group_idx] = {}\n",
    "    for label in range(len(groups[group_idx])):  \n",
    "        indecies = [i for i, label_i in enumerate(labels) if label_i == label]\n",
    "        print(indecies)\n",
    "        filtered_data_imgs = np.take(imgs, indecies, axis=0).astype(np.float32)\n",
    "    \n",
    "        latent_out, _, _ = net.encode(torch.tensor(filtered_data_imgs))\n",
    "        latent[group_idx][label] = latent_out.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([8, 8, 8, 8, 8, 4, 4, 4, 4, 4, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 7, 7,\n",
       "       7, 7, 7, 6, 6, 6, 6, 6, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 2, 2,\n",
       "       2, 5, 5, 5, 5, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 2.3691063 ,  2.0686724 ,  0.11104929,  0.7617174 ],\n",
       "       [ 2.5010726 ,  1.5249442 ,  1.1973306 , -0.98127985],\n",
       "       [ 2.67693   ,  3.0622165 , -0.0385436 ,  0.2827496 ],\n",
       "       [ 1.6710508 ,  0.8730894 , -0.37629336, -1.2154037 ],\n",
       "       [ 2.8382    ,  2.8577678 ,  0.30587205,  0.27836552]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "latent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = {'apple': 2, 'banana': 2, 'blueberry': 0, 'corn': 2, 'eggplant': 3, 'kaki': 1, 'lemon': 0, 'mango': 0, 'orange': 0, 'pear': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = test_imgs_100\n",
    "imgs = np.swapaxes(test_imgs_100, 1, 3) # (n, 3, 64, 64)\n",
    "imgs = imgs/255\n",
    "labels = test_labels_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {v.lower():k for k,v in enumerate(np.unique(test_labels_100)) }\n",
    "id_to_label = {v:k for k,v in label_to_id.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 'apple',\n",
       " 1: 'banana',\n",
       " 2: 'blueberry',\n",
       " 3: 'corn',\n",
       " 4: 'eggplant',\n",
       " 5: 'kaki',\n",
       " 6: 'lemon',\n",
       " 7: 'mango',\n",
       " 8: 'orange',\n",
       " 9: 'pear'}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "labels[0].lower()\n",
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 3, 3, 4, 4, 4, 5]"
      ]
     },
     "metadata": {},
     "execution_count": 179
    }
   ],
   "source": [
    "cc[labels[0].lower()]\n",
    "\n",
    "select_10 = []\n",
    "for (fruit, fnum) in cc.items():\n",
    "    while not fnum == 0:\n",
    "        select_10.append(label_to_id[fruit.lower()])\n",
    "        fnum -= 1\n",
    "select_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[55, 97, 120, 142, 310, 304, 496, 499, 481, 594]"
      ]
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "source": [
    "test_list = [0, 0, 1, 1, 3, 3, 4, 4, 4, 5]\n",
    "incid = [np.random.randint(0,100) + 100*i for i in test_list]\n",
    "incid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 5.0480,  0.8348, -0.1941, -0.0096],\n",
       "        [ 4.7651,  1.1600,  0.5454,  0.0488],\n",
       "        [-1.7374, -0.5336, -0.3445, -0.4323],\n",
       "        [-1.4363, -1.3078,  1.0163, -0.2258],\n",
       "        [-1.1887, -0.2771, -0.9389, -0.6221],\n",
       "        [-0.1838,  0.0749,  0.4873,  0.0754],\n",
       "        [ 1.5223,  0.8615,  0.4810, -1.2464],\n",
       "        [ 2.1577,  1.6866, -0.4839, -0.9207],\n",
       "        [ 2.4183,  2.7551,  0.1506,  0.4632],\n",
       "        [ 0.4164,  1.1250, -0.6538,  1.6784]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "filtered_data_imgs = np.take(imgs, incid, axis=0).astype(np.float32)\n",
    "latent_out, _, _ = net.encode(torch.tensor(filtered_data_imgs))\n",
    "latent_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('apple', array([ 5.0480237 ,  0.8347916 , -0.19412214, -0.00962634])),\n",
       " ('apple', array([4.76509047, 1.15995181, 0.54543334, 0.0487729 ])),\n",
       " ('banana', array([-1.73744595, -0.53363395, -0.34452271, -0.43234622])),\n",
       " ('banana', array([-1.43629956, -1.30781043,  1.01633394, -0.22584137])),\n",
       " ('corn', array([-1.18869901, -0.27705237, -0.93886608, -0.62213343])),\n",
       " ('corn', array([-0.1838163 ,  0.07486677,  0.48726872,  0.07544299])),\n",
       " ('eggplant', array([ 1.52232325,  0.86154848,  0.48096091, -1.24640119])),\n",
       " ('eggplant', array([ 2.1577189 ,  1.6866374 , -0.48394608, -0.92070025])),\n",
       " ('eggplant', array([2.41828609, 2.75510383, 0.15061529, 0.46316192])),\n",
       " ('kaki', array([ 0.41638625,  1.12501609, -0.65382934,  1.67839038]))]"
      ]
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "fruit_dic = []\n",
    "for i in range(10):\n",
    "    fruit_name = id_to_label[select_10[i]]\n",
    "    pred = latent_out[i].detach().numpy()\n",
    "    fruit_dic.append((fruit_name, np.array(pred, dtype='float')))\n",
    "fruit_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#not this#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 1]\n",
      "[2, 3]\n",
      "[]\n",
      "[4, 5]\n",
      "[6, 7, 8]\n",
      "[9]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: {0: array([[ 4.7681403 ,  0.9798317 ,  1.3957351 ,  0.09522648],\n",
       "         [ 4.7758036 ,  1.1752228 , -0.38502648,  0.12011665]],\n",
       "        dtype=float32),\n",
       "  1: array([[5.1187935 , 0.9028863 , 1.5332273 , 0.0834244 ],\n",
       "         [5.265356  , 0.9243038 , 0.8544263 , 0.24710628]], dtype=float32),\n",
       "  3: array([[ 4.897072  ,  0.88167876,  1.1739283 , -0.26181182],\n",
       "         [ 4.884778  ,  0.875858  , -0.7620531 ,  0.01083781]],\n",
       "        dtype=float32),\n",
       "  4: array([[ 5.213238  ,  1.1264787 , -1.6092746 ,  0.22209597],\n",
       "         [ 5.1575537 ,  0.9007012 ,  0.2085421 ,  0.41553402],\n",
       "         [ 5.3002877 ,  0.974637  ,  0.8561054 ,  0.43721384]],\n",
       "        dtype=float32),\n",
       "  5: array([[ 4.9558244 ,  1.2688925 , -1.5719892 ,  0.13183293]],\n",
       "        dtype=float32)}}"
      ]
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "latent = {}\n",
    "for group_idx in groups.keys():\n",
    "    latent[group_idx] = {}\n",
    "    for label in range(len(groups[group_idx])):  \n",
    "        indecies = [i for i, label_i in enumerate(select_10) if label_i == label]\n",
    "        print(indecies)\n",
    "        if  not indecies == []:\n",
    "            filtered_data_imgs = np.take(imgs, indecies, axis=0).astype(np.float32)\n",
    "            latent_out, _, _ = net.encode(torch.tensor(filtered_data_imgs))\n",
    "            latent[group_idx][label] = latent_out.detach().numpy()\n",
    "latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: {0: array([[4.869334  , 0.94339395, 0.2182904 , 0.10302754],\n",
       "         [5.049664  , 1.1624012 , 1.5175778 , 0.16589102]], dtype=float32),\n",
       "  1: array([[ 5.212882  ,  0.94136167,  1.4061406 ,  0.05412238],\n",
       "         [ 4.9311957 ,  0.91960835, -1.0083135 ,  0.23742832]],\n",
       "        dtype=float32),\n",
       "  3: array([[ 4.990582  ,  0.9827923 , -1.2049844 , -0.21127099],\n",
       "         [ 4.8523054 ,  0.9534163 , -1.3470364 ,  0.07308765]],\n",
       "        dtype=float32),\n",
       "  4: array([[ 5.1115227 ,  0.98608345, -1.0387119 ,  0.12370645],\n",
       "         [ 5.283029  ,  0.8838004 ,  0.6265168 ,  0.4067654 ],\n",
       "         [ 5.596168  ,  0.89492565, -1.2474103 ,  0.40614352]],\n",
       "        dtype=float32),\n",
       "  5: array([[5.06676   , 1.1875188 , 0.02973331, 0.21877374]], dtype=float32)}}"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('apple', array([4.7008495, 0.9251889, 0.9384835, 0.1172428], dtype=float32)),\n",
       " ('apple',\n",
       "  array([ 4.9910173 ,  1.1615291 , -1.1792406 ,  0.16839597], dtype=float32)),\n",
       " ('banana',\n",
       "  array([4.842707 , 0.9735563, 0.850629 , 0.0160583], dtype=float32)),\n",
       " ('banana',\n",
       "  array([ 5.1717334 ,  0.9563578 , -0.47481042,  0.2953953 ], dtype=float32)),\n",
       " ('corn',\n",
       "  array([ 4.773913  ,  0.8409624 , -1.4707614 , -0.28657475], dtype=float32)),\n",
       " ('corn',\n",
       "  array([ 4.5907717 ,  0.796672  , -0.45461065,  0.00692413], dtype=float32)),\n",
       " ('eggplant',\n",
       "  array([5.0864363 , 1.1268605 , 0.8535908 , 0.16164148], dtype=float32)),\n",
       " ('eggplant',\n",
       "  array([ 5.29296   ,  0.9293656 , -0.07538732,  0.3935193 ], dtype=float32)),\n",
       " ('eggplant',\n",
       "  array([5.3729153, 0.9852488, 0.3370942, 0.3613001], dtype=float32)),\n",
       " ('kaki',\n",
       "  array([4.9832363 , 1.2316736 , 0.22599924, 0.08880075], dtype=float32))]"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "fruit_dic = []\n",
    "for fruit_id, pred_list in latent[0].items():\n",
    "    for pred in pred_list:\n",
    "        fruit_name = id_to_label[fruit_id]\n",
    "        fruit_dic.append((fruit_name, pred))\n",
    "fruit_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(fruit_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['apple braeburn',\n",
       " 'banana',\n",
       " 'blueberry',\n",
       " 'corn',\n",
       " 'eggplant',\n",
       " 'kaki',\n",
       " 'lemon',\n",
       " 'mango',\n",
       " 'orange',\n",
       " 'pear']"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "list(label_to_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['apple', 'banana', 'blueberry', 'corn', 'eggplant', 'kaki', 'lemon', 'mango', 'orange', 'pear'])"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "{'apple': ['appleb', 'apple'],\n",
    "'banana' : ['banana'],\n",
    "'blueberry' : ['blueberry'],\n",
    "'corn' : ['corn'],\n",
    "'eggplant': ['eggplant'],\n",
    "'kaki': ['kaki'],\n",
    "'lemon': ['lemon'],\n",
    "'mango': ['mango'],\n",
    "'orange' : ['orange'],\n",
    "'pear': ['pear']}.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_colour_generator(mean=0, std=0):\n",
    "    \"\"\" Creates a colour generator function which generates random HSV colours based on the provided mean and std\n",
    "\n",
    "    the SV channels will always use the same mean and std, ensuring that colours are not too dark or too light\n",
    "\n",
    "    :param mean:\n",
    "    :param std:\n",
    "    :return: colour_generator function\n",
    "    \"\"\"\n",
    "    def colour_generator():\n",
    "        return np.array((((np.random.randn() * std + mean) % 360) / 3.6 , 100 - np.abs(np.random.randn() * 10), 100 - np.abs(np.random.rand() * 20)))/100\n",
    "    return colour_generator\n",
    "\n",
    "# These mean and std values seem to generate good values for each colour which all look sensibly like the specified colour\n",
    "colour_values = {\"red\": (0, 5),\n",
    "                 \"orange\": (30, 5),\n",
    "                 \"yellow\": (58, 2),\n",
    "                 \"green\": (120, 9),\n",
    "                 \"blue\": (220, 13),\n",
    "                 \"purple\": (270, 9),\n",
    "                 \"pink\": (315, 9)}\n",
    "# This dict maps a colour to its colour generator\n",
    "# So to generate red use colour_generators['red']()\n",
    "colour_generators = {colour: generate_colour_generator(*values) for colour, values in colour_values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = random.sample(colour_values.keys(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.90869884, 0.95102625, 0.91671468])"
      ]
     },
     "metadata": {},
     "execution_count": 212
    }
   ],
   "source": [
    "colour_generators[sss[0]]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_dic = []\n",
    "for i in range(10):\n",
    "    fruit_name = id_to_label[select_10[i]]\n",
    "    pred = latent_out[i].detach().numpy()\n",
    "    colour = random.sample(colour_values.keys(), 1)\n",
    "    long_data = np.hstack((pred, colour_generators[colour[0]]()))\n",
    "    fruit_dic.append((fruit_name, np.array(long_data, dtype='float')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('apple',\n",
       "  array([ 5.0480237 ,  0.8347916 , -0.19412214, -0.00962634,  0.57904829,\n",
       "          0.85254352,  0.86180451])),\n",
       " ('apple',\n",
       "  array([4.76509047, 1.15995181, 0.54543334, 0.0487729 , 0.00695483,\n",
       "         0.888134  , 0.98691595])),\n",
       " ('banana',\n",
       "  array([-1.73744595, -0.53363395, -0.34452271, -0.43234622,  0.63658747,\n",
       "          0.99990141,  0.88476406])),\n",
       " ('banana',\n",
       "  array([-1.43629956, -1.30781043,  1.01633394, -0.22584137,  0.1557267 ,\n",
       "          0.94510291,  0.98111312])),\n",
       " ('corn',\n",
       "  array([-1.18869901, -0.27705237, -0.93886608, -0.62213343,  0.30646622,\n",
       "          0.79944076,  0.82605357])),\n",
       " ('corn',\n",
       "  array([-0.1838163 ,  0.07486677,  0.48726872,  0.07544299,  0.00940821,\n",
       "          0.99968653,  0.9864536 ])),\n",
       " ('eggplant',\n",
       "  array([ 1.52232325,  0.86154848,  0.48096091, -1.24640119,  0.01762886,\n",
       "          0.9500535 ,  0.95823115])),\n",
       " ('eggplant',\n",
       "  array([ 2.1577189 ,  1.6866374 , -0.48394608, -0.92070025,  0.73739797,\n",
       "          0.98862877,  0.96928054])),\n",
       " ('eggplant',\n",
       "  array([2.41828609, 2.75510383, 0.15061529, 0.46316192, 0.832315  ,\n",
       "         0.87886673, 0.9079469 ])),\n",
       " ('kaki',\n",
       "  array([ 0.41638625,  1.12501609, -0.65382934,  1.67839038,  0.07337748,\n",
       "          0.99490664,  0.99270161]))]"
      ]
     },
     "metadata": {},
     "execution_count": 222
    }
   ],
   "source": [
    "fruit_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}