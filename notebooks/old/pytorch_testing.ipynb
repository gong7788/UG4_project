{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 23752898.0\n",
      "1 16838880.0\n",
      "2 13124202.0\n",
      "3 10682584.0\n",
      "4 8768808.0\n",
      "5 7127164.5\n",
      "6 5682423.0\n",
      "7 4449930.5\n",
      "8 3434524.25\n",
      "9 2631951.25\n",
      "10 2012700.875\n",
      "11 1546348.5\n",
      "12 1198327.5\n",
      "13 940111.8125\n",
      "14 747551.3125\n",
      "15 603231.375\n",
      "16 493741.6875\n",
      "17 409893.625\n",
      "18 344634.125\n",
      "19 293174.25\n",
      "20 251877.984375\n",
      "21 218282.75\n",
      "22 190577.84375\n",
      "23 167504.09375\n",
      "24 148049.5\n",
      "25 131484.359375\n",
      "26 117279.03125\n",
      "27 105005.7109375\n",
      "28 94353.125\n",
      "29 85021.5078125\n",
      "30 76812.7734375\n",
      "31 69559.53125\n",
      "32 63131.92578125\n",
      "33 57414.0703125\n",
      "34 52315.484375\n",
      "35 47750.08203125\n",
      "36 43651.59375\n",
      "37 39963.390625\n",
      "38 36640.71875\n",
      "39 33638.87109375\n",
      "40 30923.888671875\n",
      "41 28462.931640625\n",
      "42 26228.046875\n",
      "43 24201.375\n",
      "44 22356.931640625\n",
      "45 20675.953125\n",
      "46 19138.091796875\n",
      "47 17730.728515625\n",
      "48 16441.802734375\n",
      "49 15260.6689453125\n",
      "50 14175.939453125\n",
      "51 13178.365234375\n",
      "52 12261.521484375\n",
      "53 11416.15625\n",
      "54 10635.3544921875\n",
      "55 9914.1982421875\n",
      "56 9247.66796875\n",
      "57 8631.095703125\n",
      "58 8059.904296875\n",
      "59 7530.8173828125\n",
      "60 7039.208984375\n",
      "61 6583.15478515625\n",
      "62 6159.65673828125\n",
      "63 5766.0302734375\n",
      "64 5399.83056640625\n",
      "65 5059.037109375\n",
      "66 4741.7216796875\n",
      "67 4446.138671875\n",
      "68 4170.33837890625\n",
      "69 3913.072509765625\n",
      "70 3672.9150390625\n",
      "71 3448.57763671875\n",
      "72 3239.049072265625\n",
      "73 3043.0537109375\n",
      "74 2859.8017578125\n",
      "75 2688.36669921875\n",
      "76 2527.849365234375\n",
      "77 2377.76708984375\n",
      "78 2237.1455078125\n",
      "79 2105.474853515625\n",
      "80 1982.053466796875\n",
      "81 1866.280029296875\n",
      "82 1757.6695556640625\n",
      "83 1655.77587890625\n",
      "84 1560.0667724609375\n",
      "85 1470.1783447265625\n",
      "86 1385.7086181640625\n",
      "87 1306.338134765625\n",
      "88 1231.7335205078125\n",
      "89 1161.6175537109375\n",
      "90 1095.6563720703125\n",
      "91 1033.60791015625\n",
      "92 975.2208251953125\n",
      "93 920.2957153320312\n",
      "94 868.5906982421875\n",
      "95 819.8999633789062\n",
      "96 774.0618286132812\n",
      "97 730.8683471679688\n",
      "98 690.1771240234375\n",
      "99 651.8377075195312\n",
      "100 615.7117919921875\n",
      "101 581.6620483398438\n",
      "102 549.5616455078125\n",
      "103 519.2861938476562\n",
      "104 490.75262451171875\n",
      "105 463.8647155761719\n",
      "106 438.47589111328125\n",
      "107 414.51763916015625\n",
      "108 391.9260559082031\n",
      "109 370.5872802734375\n",
      "110 350.4407958984375\n",
      "111 331.4348449707031\n",
      "112 313.4759521484375\n",
      "113 296.5300598144531\n",
      "114 280.5206604003906\n",
      "115 265.398193359375\n",
      "116 251.11549377441406\n",
      "117 237.619140625\n",
      "118 224.87179565429688\n",
      "119 212.8294677734375\n",
      "120 201.4521484375\n",
      "121 190.69979858398438\n",
      "122 180.5331573486328\n",
      "123 170.91726684570312\n",
      "124 161.83407592773438\n",
      "125 153.23876953125\n",
      "126 145.1126708984375\n",
      "127 137.42738342285156\n",
      "128 130.15721130371094\n",
      "129 123.28471374511719\n",
      "130 116.7802734375\n",
      "131 110.62850189208984\n",
      "132 104.80835723876953\n",
      "133 99.29942321777344\n",
      "134 94.08892822265625\n",
      "135 89.15473175048828\n",
      "136 84.48595428466797\n",
      "137 80.06803131103516\n",
      "138 75.88477325439453\n",
      "139 71.9236068725586\n",
      "140 68.17308807373047\n",
      "141 64.62217712402344\n",
      "142 61.26013946533203\n",
      "143 58.075958251953125\n",
      "144 55.06177520751953\n",
      "145 52.20711898803711\n",
      "146 49.5028190612793\n",
      "147 46.93952941894531\n",
      "148 44.51352310180664\n",
      "149 42.214202880859375\n",
      "150 40.035400390625\n",
      "151 37.971778869628906\n",
      "152 36.016204833984375\n",
      "153 34.16318130493164\n",
      "154 32.407958984375\n",
      "155 30.74289321899414\n",
      "156 29.165542602539062\n",
      "157 27.670236587524414\n",
      "158 26.253870010375977\n",
      "159 24.91031837463379\n",
      "160 23.63593101501465\n",
      "161 22.42863655090332\n",
      "162 21.28478240966797\n",
      "163 20.19947052001953\n",
      "164 19.169960021972656\n",
      "165 18.194717407226562\n",
      "166 17.269248962402344\n",
      "167 16.391878128051758\n",
      "168 15.559863090515137\n",
      "169 14.770638465881348\n",
      "170 14.021706581115723\n",
      "171 13.311307907104492\n",
      "172 12.637432098388672\n",
      "173 11.998434066772461\n",
      "174 11.391873359680176\n",
      "175 10.816332817077637\n",
      "176 10.27042293548584\n",
      "177 9.752779006958008\n",
      "178 9.26119613647461\n",
      "179 8.794682502746582\n",
      "180 8.352334022521973\n",
      "181 7.932644367218018\n",
      "182 7.534246921539307\n",
      "183 7.155972003936768\n",
      "184 6.797030925750732\n",
      "185 6.456283092498779\n",
      "186 6.1327056884765625\n",
      "187 5.825801849365234\n",
      "188 5.534063339233398\n",
      "189 5.2573957443237305\n",
      "190 4.994836807250977\n",
      "191 4.745347023010254\n",
      "192 4.508606910705566\n",
      "193 4.28387975692749\n",
      "194 4.070520877838135\n",
      "195 3.8677940368652344\n",
      "196 3.6752140522003174\n",
      "197 3.4923465251922607\n",
      "198 3.3188066482543945\n",
      "199 3.153794288635254\n",
      "200 2.99725341796875\n",
      "201 2.848188877105713\n",
      "202 2.707125663757324\n",
      "203 2.5728962421417236\n",
      "204 2.445526599884033\n",
      "205 2.3242416381835938\n",
      "206 2.209266424179077\n",
      "207 2.1001009941101074\n",
      "208 1.9960788488388062\n",
      "209 1.8975751399993896\n",
      "210 1.8039225339889526\n",
      "211 1.7148925065994263\n",
      "212 1.6301591396331787\n",
      "213 1.5500484704971313\n",
      "214 1.4734165668487549\n",
      "215 1.4010001420974731\n",
      "216 1.3319995403289795\n",
      "217 1.2664958238601685\n",
      "218 1.2041449546813965\n",
      "219 1.1449060440063477\n",
      "220 1.0887030363082886\n",
      "221 1.0351184606552124\n",
      "222 0.9842948317527771\n",
      "223 0.9360828399658203\n",
      "224 0.8901268839836121\n",
      "225 0.8465006351470947\n",
      "226 0.8049924969673157\n",
      "227 0.7655810713768005\n",
      "228 0.7281615138053894\n",
      "229 0.6924874186515808\n",
      "230 0.6585884690284729\n",
      "231 0.6264455318450928\n",
      "232 0.5957908034324646\n",
      "233 0.5666660666465759\n",
      "234 0.5390139222145081\n",
      "235 0.5127177834510803\n",
      "236 0.4877127408981323\n",
      "237 0.46403104066848755\n",
      "238 0.4413294196128845\n",
      "239 0.4198738634586334\n",
      "240 0.39930447936058044\n",
      "241 0.37991243600845337\n",
      "242 0.3614538311958313\n",
      "243 0.3438390791416168\n",
      "244 0.3271060585975647\n",
      "245 0.31117770075798035\n",
      "246 0.2960711419582367\n",
      "247 0.28168076276779175\n",
      "248 0.26799583435058594\n",
      "249 0.25500160455703735\n",
      "250 0.2426120936870575\n",
      "251 0.2308722734451294\n",
      "252 0.21961773931980133\n",
      "253 0.20900025963783264\n",
      "254 0.19891676306724548\n",
      "255 0.18931156396865845\n",
      "256 0.1800684928894043\n",
      "257 0.17133434116840363\n",
      "258 0.16305482387542725\n",
      "259 0.15520969033241272\n",
      "260 0.14764364063739777\n",
      "261 0.1405707597732544\n",
      "262 0.13373695313930511\n",
      "263 0.12729163467884064\n",
      "264 0.12112431228160858\n",
      "265 0.1152726411819458\n",
      "266 0.10970484465360641\n",
      "267 0.10442867875099182\n",
      "268 0.09936946630477905\n",
      "269 0.09461440145969391\n",
      "270 0.0900462195277214\n",
      "271 0.08571728318929672\n",
      "272 0.08158867806196213\n",
      "273 0.07766812294721603\n",
      "274 0.07395825535058975\n",
      "275 0.07038883864879608\n",
      "276 0.06699495762586594\n",
      "277 0.06374984234571457\n",
      "278 0.06069285795092583\n",
      "279 0.05779474228620529\n",
      "280 0.05500756949186325\n",
      "281 0.05236649885773659\n",
      "282 0.04986187070608139\n",
      "283 0.047460250556468964\n",
      "284 0.04519757628440857\n",
      "285 0.04302862659096718\n",
      "286 0.04095972329378128\n",
      "287 0.03900647535920143\n",
      "288 0.03713051974773407\n",
      "289 0.035366300493478775\n",
      "290 0.033685360103845596\n",
      "291 0.03206927329301834\n",
      "292 0.030549079179763794\n",
      "293 0.02908596396446228\n",
      "294 0.027693189680576324\n",
      "295 0.02636615000665188\n",
      "296 0.025104200467467308\n",
      "297 0.02391624078154564\n",
      "298 0.022791463881731033\n",
      "299 0.021710174158215523\n",
      "300 0.0206785686314106\n",
      "301 0.019701674580574036\n",
      "302 0.01875942014157772\n",
      "303 0.017884910106658936\n",
      "304 0.017033152282238007\n",
      "305 0.016229113563895226\n",
      "306 0.015464305877685547\n",
      "307 0.014743989333510399\n",
      "308 0.014047038741409779\n",
      "309 0.013383443467319012\n",
      "310 0.0127576794475317\n",
      "311 0.012156479060649872\n",
      "312 0.011588365770876408\n",
      "313 0.011051260866224766\n",
      "314 0.01054165605455637\n",
      "315 0.010047908872365952\n",
      "316 0.009575764648616314\n",
      "317 0.00912853516638279\n",
      "318 0.008707279339432716\n",
      "319 0.008302656002342701\n",
      "320 0.007920718751847744\n",
      "321 0.007559658493846655\n",
      "322 0.0072104898281395435\n",
      "323 0.006880458444356918\n",
      "324 0.006566009484231472\n",
      "325 0.00626412546262145\n",
      "326 0.005979285109788179\n",
      "327 0.0057066092267632484\n",
      "328 0.005449786316603422\n",
      "329 0.005210126284509897\n",
      "330 0.00496927322819829\n",
      "331 0.004747668281197548\n",
      "332 0.004538007080554962\n",
      "333 0.0043344381265342236\n",
      "334 0.004143176134675741\n",
      "335 0.003961177077144384\n",
      "336 0.0037829906214028597\n",
      "337 0.0036157576832920313\n",
      "338 0.003459810046479106\n",
      "339 0.0033046139869838953\n",
      "340 0.0031592436134815216\n",
      "341 0.0030242237262427807\n",
      "342 0.002897349651902914\n",
      "343 0.0027769734151661396\n",
      "344 0.002659143880009651\n",
      "345 0.002543768845498562\n",
      "346 0.0024364301934838295\n",
      "347 0.0023323397617787123\n",
      "348 0.002232097089290619\n",
      "349 0.0021392409689724445\n",
      "350 0.0020500083919614553\n",
      "351 0.00196533533744514\n",
      "352 0.0018806509906426072\n",
      "353 0.0018026219913735986\n",
      "354 0.001731302123516798\n",
      "355 0.0016602647956460714\n",
      "356 0.001594153931364417\n",
      "357 0.0015331943286582828\n",
      "358 0.0014704008353874087\n",
      "359 0.001409526215866208\n",
      "360 0.0013546572299674153\n",
      "361 0.0013023893116042018\n",
      "362 0.0012517363065853715\n",
      "363 0.0012025063624605536\n",
      "364 0.001154532190412283\n",
      "365 0.0011111493222415447\n",
      "366 0.0010687860194593668\n",
      "367 0.0010270465863868594\n",
      "368 0.000988887855783105\n",
      "369 0.0009539041784591973\n",
      "370 0.0009192469879053533\n",
      "371 0.0008865756099112332\n",
      "372 0.0008543775766156614\n",
      "373 0.0008249366655945778\n",
      "374 0.0007963557727634907\n",
      "375 0.0007680708076804876\n",
      "376 0.0007406154763884842\n",
      "377 0.0007144690025597811\n",
      "378 0.0006902010645717382\n",
      "379 0.0006673551979474723\n",
      "380 0.0006446579354815185\n",
      "381 0.0006211395957507193\n",
      "382 0.0005997508415021002\n",
      "383 0.0005813442985527217\n",
      "384 0.0005627812934108078\n",
      "385 0.0005438592634163797\n",
      "386 0.0005258888704702258\n",
      "387 0.0005095023079775274\n",
      "388 0.0004928252892568707\n",
      "389 0.00047765503404662013\n",
      "390 0.0004617283120751381\n",
      "391 0.00044702726881951094\n",
      "392 0.00043340626871213317\n",
      "393 0.0004185959987808019\n",
      "394 0.00040642861858941615\n",
      "395 0.00039549299981445074\n",
      "396 0.00038305154885165393\n",
      "397 0.0003715045459102839\n",
      "398 0.0003605772217269987\n",
      "399 0.00035127883893437684\n",
      "400 0.0003411129873711616\n",
      "401 0.0003302805998828262\n",
      "402 0.000321058789268136\n",
      "403 0.0003117629967164248\n",
      "404 0.00030208940734155476\n",
      "405 0.0002944371080957353\n",
      "406 0.0002865427522920072\n",
      "407 0.0002789663849398494\n",
      "408 0.00027149487868882716\n",
      "409 0.0002640463935676962\n",
      "410 0.00025708714383654296\n",
      "411 0.00025082906358875334\n",
      "412 0.00024379085516557097\n",
      "413 0.00023710001551080495\n",
      "414 0.00023183182929642498\n",
      "415 0.0002264004579046741\n",
      "416 0.00022023280325811356\n",
      "417 0.0002146641636500135\n",
      "418 0.00020855283946730196\n",
      "419 0.0002033437049249187\n",
      "420 0.00019863310444634408\n",
      "421 0.00019364642503205687\n",
      "422 0.00018832460045814514\n",
      "423 0.00018400122644379735\n",
      "424 0.00017919467063620687\n",
      "425 0.00017565842426847667\n",
      "426 0.00017151316569652408\n",
      "427 0.00016763732128310949\n",
      "428 0.00016406348731834441\n",
      "429 0.0001598103845026344\n",
      "430 0.00015658426855225116\n",
      "431 0.00015303632244467735\n",
      "432 0.0001499758509453386\n",
      "433 0.00014637444110121578\n",
      "434 0.00014311957056634128\n",
      "435 0.00014066783478483558\n",
      "436 0.00013712617510464042\n",
      "437 0.00013431560364551842\n",
      "438 0.00013162585673853755\n",
      "439 0.00012924779730383307\n",
      "440 0.0001265589817194268\n",
      "441 0.00012376077938824892\n",
      "442 0.00012089952360838652\n",
      "443 0.00011823505337815732\n",
      "444 0.00011595922114793211\n",
      "445 0.00011349093983881176\n",
      "446 0.00011102553980890661\n",
      "447 0.00010896349704125896\n",
      "448 0.00010655893129296601\n",
      "449 0.00010470972483744845\n",
      "450 0.0001026750251185149\n",
      "451 0.00010087413102155551\n",
      "452 9.909559594234452e-05\n",
      "453 9.704713011160493e-05\n",
      "454 9.527873044135049e-05\n",
      "455 9.354155190521851e-05\n",
      "456 9.184494410874322e-05\n",
      "457 9.039379801834002e-05\n",
      "458 8.901214459910989e-05\n",
      "459 8.705048094270751e-05\n",
      "460 8.56324695632793e-05\n",
      "461 8.424247789662331e-05\n",
      "462 8.245401841122657e-05\n",
      "463 8.137893746607006e-05\n",
      "464 7.98504042904824e-05\n",
      "465 7.809144881321117e-05\n",
      "466 7.689638005103916e-05\n",
      "467 7.562259997939691e-05\n",
      "468 7.391776307485998e-05\n",
      "469 7.268977060448378e-05\n",
      "470 7.142699905671179e-05\n",
      "471 7.014949369477108e-05\n",
      "472 6.912159005878493e-05\n",
      "473 6.793544889660552e-05\n",
      "474 6.69914879836142e-05\n",
      "475 6.601181667065248e-05\n",
      "476 6.459133146563545e-05\n",
      "477 6.368535832734779e-05\n",
      "478 6.266776472330093e-05\n",
      "479 6.186891550896689e-05\n",
      "480 6.067663343856111e-05\n",
      "481 6.011179357301444e-05\n",
      "482 5.9084606618853286e-05\n",
      "483 5.792778392788023e-05\n",
      "484 5.725307346438058e-05\n",
      "485 5.632931788568385e-05\n",
      "486 5.56529157620389e-05\n",
      "487 5.462661283672787e-05\n",
      "488 5.388801218941808e-05\n",
      "489 5.307662286213599e-05\n",
      "490 5.2492647228064016e-05\n",
      "491 5.160710497875698e-05\n",
      "492 5.077368041384034e-05\n",
      "493 4.99356392538175e-05\n",
      "494 4.944626925862394e-05\n",
      "495 4.89118137920741e-05\n",
      "496 4.8064710426842794e-05\n",
      "497 4.7262743464671075e-05\n",
      "498 4.6834935346851125e-05\n",
      "499 4.60438932350371e-05\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31206054.0\n",
      "1 25461074.0\n",
      "2 24892312.0\n",
      "3 25100538.0\n",
      "4 23465350.0\n",
      "5 19036628.0\n",
      "6 13305037.0\n",
      "7 8241092.5\n",
      "8 4828962.5\n",
      "9 2865948.75\n",
      "10 1814080.875\n",
      "11 1249799.25\n",
      "12 932117.0\n",
      "13 738559.6875\n",
      "14 609588.0\n",
      "15 516414.09375\n",
      "16 444632.53125\n",
      "17 387013.15625\n",
      "18 339524.75\n",
      "19 299705.75\n",
      "20 265867.125\n",
      "21 236829.84375\n",
      "22 211719.921875\n",
      "23 189898.15625\n",
      "24 170840.84375\n",
      "25 154245.484375\n",
      "26 139632.8125\n",
      "27 126704.734375\n",
      "28 115238.4921875\n",
      "29 105024.640625\n",
      "30 95900.0546875\n",
      "31 87734.90625\n",
      "32 80393.140625\n",
      "33 73782.234375\n",
      "34 67814.8203125\n",
      "35 62415.41015625\n",
      "36 57527.17578125\n",
      "37 53094.9921875\n",
      "38 49059.7421875\n",
      "39 45387.0546875\n",
      "40 42033.0078125\n",
      "41 38966.62890625\n",
      "42 36162.69140625\n",
      "43 33593.296875\n",
      "44 31234.595703125\n",
      "45 29066.802734375\n",
      "46 27069.75\n",
      "47 25227.17578125\n",
      "48 23526.171875\n",
      "49 21953.875\n",
      "50 20498.923828125\n",
      "51 19152.78125\n",
      "52 17904.01171875\n",
      "53 16746.921875\n",
      "54 15672.5966796875\n",
      "55 14674.455078125\n",
      "56 13746.029296875\n",
      "57 12882.326171875\n",
      "58 12078.787109375\n",
      "59 11330.224609375\n",
      "60 10636.4453125\n",
      "61 9988.5634765625\n",
      "62 9383.759765625\n",
      "63 8818.4404296875\n",
      "64 8290.14453125\n",
      "65 7796.04931640625\n",
      "66 7333.5849609375\n",
      "67 6900.51171875\n",
      "68 6494.92822265625\n",
      "69 6115.08203125\n",
      "70 5759.03076171875\n",
      "71 5425.1279296875\n",
      "72 5111.83740234375\n",
      "73 4817.9658203125\n",
      "74 4542.01904296875\n",
      "75 4282.87890625\n",
      "76 4039.448486328125\n",
      "77 3810.7255859375\n",
      "78 3595.6904296875\n",
      "79 3393.72998046875\n",
      "80 3203.9189453125\n",
      "81 3025.300048828125\n",
      "82 2857.16064453125\n",
      "83 2698.94287109375\n",
      "84 2549.86669921875\n",
      "85 2409.470703125\n",
      "86 2277.2138671875\n",
      "87 2152.641357421875\n",
      "88 2035.34130859375\n",
      "89 1924.966064453125\n",
      "90 1820.91748046875\n",
      "91 1722.764404296875\n",
      "92 1630.166259765625\n",
      "93 1542.75146484375\n",
      "94 1460.2462158203125\n",
      "95 1382.3515625\n",
      "96 1308.7978515625\n",
      "97 1239.364013671875\n",
      "98 1173.7471923828125\n",
      "99 1111.73681640625\n",
      "100 1053.148193359375\n",
      "101 997.7767944335938\n",
      "102 945.44189453125\n",
      "103 895.9520874023438\n",
      "104 849.1571044921875\n",
      "105 804.9033813476562\n",
      "106 763.0408935546875\n",
      "107 723.4395751953125\n",
      "108 685.9694213867188\n",
      "109 650.5061645507812\n",
      "110 616.947998046875\n",
      "111 585.1964721679688\n",
      "112 555.1243896484375\n",
      "113 526.6616821289062\n",
      "114 499.703369140625\n",
      "115 474.1727294921875\n",
      "116 450.0026550292969\n",
      "117 427.1035461425781\n",
      "118 405.42218017578125\n",
      "119 384.8681640625\n",
      "120 365.4347839355469\n",
      "121 347.0208740234375\n",
      "122 329.55780029296875\n",
      "123 313.0110778808594\n",
      "124 297.316650390625\n",
      "125 282.4374694824219\n",
      "126 268.32806396484375\n",
      "127 254.9440155029297\n",
      "128 242.2494354248047\n",
      "129 230.21034240722656\n",
      "130 218.786865234375\n",
      "131 207.9495849609375\n",
      "132 197.66860961914062\n",
      "133 187.9091339111328\n",
      "134 178.6515655517578\n",
      "135 169.85826110839844\n",
      "136 161.51234436035156\n",
      "137 153.59158325195312\n",
      "138 146.06939697265625\n",
      "139 138.9278564453125\n",
      "140 132.14646911621094\n",
      "141 125.7046127319336\n",
      "142 119.58572387695312\n",
      "143 113.77580261230469\n",
      "144 108.255859375\n",
      "145 103.01127624511719\n",
      "146 98.02967071533203\n",
      "147 93.2939453125\n",
      "148 88.79364776611328\n",
      "149 84.5178451538086\n",
      "150 80.45294952392578\n",
      "151 76.5910873413086\n",
      "152 72.91895294189453\n",
      "153 69.42827606201172\n",
      "154 66.11016082763672\n",
      "155 62.9531364440918\n",
      "156 59.95345687866211\n",
      "157 57.099884033203125\n",
      "158 54.385223388671875\n",
      "159 51.80255889892578\n",
      "160 49.34878158569336\n",
      "161 47.01310729980469\n",
      "162 44.791709899902344\n",
      "163 42.67790603637695\n",
      "164 40.6656379699707\n",
      "165 38.75212860107422\n",
      "166 36.930084228515625\n",
      "167 35.19644546508789\n",
      "168 33.546878814697266\n",
      "169 31.976701736450195\n",
      "170 30.481685638427734\n",
      "171 29.058868408203125\n",
      "172 27.703758239746094\n",
      "173 26.414005279541016\n",
      "174 25.185632705688477\n",
      "175 24.016138076782227\n",
      "176 22.90239143371582\n",
      "177 21.84185791015625\n",
      "178 20.832223892211914\n",
      "179 19.869426727294922\n",
      "180 18.953697204589844\n",
      "181 18.080345153808594\n",
      "182 17.248483657836914\n",
      "183 16.455747604370117\n",
      "184 15.700292587280273\n",
      "185 14.980746269226074\n",
      "186 14.29496955871582\n",
      "187 13.64157485961914\n",
      "188 13.01919174194336\n",
      "189 12.42524528503418\n",
      "190 11.858624458312988\n",
      "191 11.319110870361328\n",
      "192 10.805280685424805\n",
      "193 10.315072059631348\n",
      "194 9.84748649597168\n",
      "195 9.401666641235352\n",
      "196 8.976973533630371\n",
      "197 8.57177734375\n",
      "198 8.185226440429688\n",
      "199 7.816527843475342\n",
      "200 7.464966297149658\n",
      "201 7.129573345184326\n",
      "202 6.80946159362793\n",
      "203 6.50468111038208\n",
      "204 6.213560104370117\n",
      "205 5.935472011566162\n",
      "206 5.670527935028076\n",
      "207 5.4177045822143555\n",
      "208 5.17637300491333\n",
      "209 4.9462714195251465\n",
      "210 4.726006984710693\n",
      "211 4.516316890716553\n",
      "212 4.316189289093018\n",
      "213 4.124953269958496\n",
      "214 3.9425978660583496\n",
      "215 3.7685086727142334\n",
      "216 3.601907253265381\n",
      "217 3.4433398246765137\n",
      "218 3.2916910648345947\n",
      "219 3.1469902992248535\n",
      "220 3.008662223815918\n",
      "221 2.876763105392456\n",
      "222 2.750678062438965\n",
      "223 2.6303040981292725\n",
      "224 2.5151779651641846\n",
      "225 2.4053425788879395\n",
      "226 2.300515651702881\n",
      "227 2.2003214359283447\n",
      "228 2.104487419128418\n",
      "229 2.0129058361053467\n",
      "230 1.9254275560379028\n",
      "231 1.8419718742370605\n",
      "232 1.7621841430664062\n",
      "233 1.6858973503112793\n",
      "234 1.6129025220870972\n",
      "235 1.5433259010314941\n",
      "236 1.4767491817474365\n",
      "237 1.4130873680114746\n",
      "238 1.3523871898651123\n",
      "239 1.2942783832550049\n",
      "240 1.2385612726211548\n",
      "241 1.1855154037475586\n",
      "242 1.1347442865371704\n",
      "243 1.0860596895217896\n",
      "244 1.0395700931549072\n",
      "245 0.9952970147132874\n",
      "246 0.9527716636657715\n",
      "247 0.9120781421661377\n",
      "248 0.8732613325119019\n",
      "249 0.836134135723114\n",
      "250 0.8006008863449097\n",
      "251 0.7666149139404297\n",
      "252 0.734140932559967\n",
      "253 0.7030203938484192\n",
      "254 0.6732621788978577\n",
      "255 0.6447999477386475\n",
      "256 0.6176491379737854\n",
      "257 0.5915383696556091\n",
      "258 0.5666082501411438\n",
      "259 0.5428158044815063\n",
      "260 0.5199970006942749\n",
      "261 0.4980687201023102\n",
      "262 0.4771575629711151\n",
      "263 0.4571413993835449\n",
      "264 0.4380364716053009\n",
      "265 0.4196550250053406\n",
      "266 0.4021301567554474\n",
      "267 0.38532954454421997\n",
      "268 0.3692946135997772\n",
      "269 0.3539304733276367\n",
      "270 0.33911845088005066\n",
      "271 0.3250289559364319\n",
      "272 0.31156453490257263\n",
      "273 0.2986341714859009\n",
      "274 0.28620582818984985\n",
      "275 0.2743942141532898\n",
      "276 0.2630470097064972\n",
      "277 0.25212985277175903\n",
      "278 0.24173663556575775\n",
      "279 0.23176360130310059\n",
      "280 0.22219091653823853\n",
      "281 0.21305085718631744\n",
      "282 0.20427657663822174\n",
      "283 0.19588056206703186\n",
      "284 0.18782511353492737\n",
      "285 0.1801111251115799\n",
      "286 0.172744482755661\n",
      "287 0.16566210985183716\n",
      "288 0.15890583395957947\n",
      "289 0.15240968763828278\n",
      "290 0.1461867392063141\n",
      "291 0.14020448923110962\n",
      "292 0.1344783902168274\n",
      "293 0.12900039553642273\n",
      "294 0.12374096363782883\n",
      "295 0.11870185285806656\n",
      "296 0.11389989405870438\n",
      "297 0.1092747300863266\n",
      "298 0.10484263300895691\n",
      "299 0.10060451924800873\n",
      "300 0.09653742611408234\n",
      "301 0.09266400337219238\n",
      "302 0.08890210092067719\n",
      "303 0.08530968427658081\n",
      "304 0.08187361806631088\n",
      "305 0.07857319712638855\n",
      "306 0.07541075348854065\n",
      "307 0.07238099724054337\n",
      "308 0.06947595626115799\n",
      "309 0.06668644398450851\n",
      "310 0.06399688869714737\n",
      "311 0.06145007163286209\n",
      "312 0.05898888036608696\n",
      "313 0.056637898087501526\n",
      "314 0.0543762743473053\n",
      "315 0.052205171436071396\n",
      "316 0.05012309178709984\n",
      "317 0.0481383353471756\n",
      "318 0.04621181637048721\n",
      "319 0.044364288449287415\n",
      "320 0.042606621980667114\n",
      "321 0.04090425744652748\n",
      "322 0.03928350284695625\n",
      "323 0.037740420550107956\n",
      "324 0.036245349794626236\n",
      "325 0.03481144458055496\n",
      "326 0.03343977406620979\n",
      "327 0.032136864960193634\n",
      "328 0.030859142541885376\n",
      "329 0.029654663056135178\n",
      "330 0.028487320989370346\n",
      "331 0.027361426502466202\n",
      "332 0.02629244327545166\n",
      "333 0.025261664763092995\n",
      "334 0.024282831698656082\n",
      "335 0.023334402590990067\n",
      "336 0.022429291158914566\n",
      "337 0.02155526727437973\n",
      "338 0.02071724459528923\n",
      "339 0.019915534183382988\n",
      "340 0.019132012501358986\n",
      "341 0.018389683216810226\n",
      "342 0.017684495076537132\n",
      "343 0.01700180023908615\n",
      "344 0.016349876299500465\n",
      "345 0.015709593892097473\n",
      "346 0.015110068023204803\n",
      "347 0.014527314342558384\n",
      "348 0.013970735482871532\n",
      "349 0.013441447168588638\n",
      "350 0.012929827906191349\n",
      "351 0.012429237365722656\n",
      "352 0.011956874281167984\n",
      "353 0.011507068760693073\n",
      "354 0.011072134599089622\n",
      "355 0.010653956793248653\n",
      "356 0.010256361216306686\n",
      "357 0.009874424897134304\n",
      "358 0.009501466527581215\n",
      "359 0.009146109223365784\n",
      "360 0.008805890567600727\n",
      "361 0.008479383774101734\n",
      "362 0.008158521726727486\n",
      "363 0.007862063124775887\n",
      "364 0.007559587713330984\n",
      "365 0.007289981935173273\n",
      "366 0.007022997364401817\n",
      "367 0.006767229177057743\n",
      "368 0.006520316004753113\n",
      "369 0.006280004512518644\n",
      "370 0.006054261699318886\n",
      "371 0.00583233404904604\n",
      "372 0.0056228660978376865\n",
      "373 0.005420391447842121\n",
      "374 0.005224298685789108\n",
      "375 0.005043407436460257\n",
      "376 0.004862653091549873\n",
      "377 0.004691107664257288\n",
      "378 0.0045267497189342976\n",
      "379 0.004370131064206362\n",
      "380 0.004211604595184326\n",
      "381 0.004067195579409599\n",
      "382 0.003925856668502092\n",
      "383 0.003791314549744129\n",
      "384 0.003661057213321328\n",
      "385 0.003536694450303912\n",
      "386 0.0034127170220017433\n",
      "387 0.003297339426353574\n",
      "388 0.0031833876855671406\n",
      "389 0.003078819951042533\n",
      "390 0.0029752603732049465\n",
      "391 0.002875245176255703\n",
      "392 0.002780976938083768\n",
      "393 0.002689279383048415\n",
      "394 0.0026013224851340055\n",
      "395 0.0025158184580504894\n",
      "396 0.0024344725534319878\n",
      "397 0.00235853623598814\n",
      "398 0.0022808020003139973\n",
      "399 0.0022074703592807055\n",
      "400 0.002136827679350972\n",
      "401 0.0020715014543384314\n",
      "402 0.0020075307693332434\n",
      "403 0.0019437741721048951\n",
      "404 0.0018819411052390933\n",
      "405 0.0018238842021673918\n",
      "406 0.001767941634170711\n",
      "407 0.0017144529847428203\n",
      "408 0.001664404640905559\n",
      "409 0.0016125165857374668\n",
      "410 0.0015653374139219522\n",
      "411 0.0015212215948849916\n",
      "412 0.0014750605914741755\n",
      "413 0.0014298546593636274\n",
      "414 0.0013883436331525445\n",
      "415 0.0013488931581377983\n",
      "416 0.0013115154579281807\n",
      "417 0.0012719308724626899\n",
      "418 0.0012358707608655095\n",
      "419 0.001202271436341107\n",
      "420 0.0011691965628415346\n",
      "421 0.0011364879319444299\n",
      "422 0.0011037973454222083\n",
      "423 0.0010743458988144994\n",
      "424 0.0010441100457683206\n",
      "425 0.0010173579212278128\n",
      "426 0.0009900007862597704\n",
      "427 0.0009628545376472175\n",
      "428 0.0009385800804011524\n",
      "429 0.0009147406090050936\n",
      "430 0.0008897898369468749\n",
      "431 0.0008662971667945385\n",
      "432 0.0008437163196504116\n",
      "433 0.0008212379761971533\n",
      "434 0.0008021000539883971\n",
      "435 0.0007809526287019253\n",
      "436 0.0007616875227540731\n",
      "437 0.0007439081091433764\n",
      "438 0.0007250114576891065\n",
      "439 0.0007065023528411984\n",
      "440 0.000688791973516345\n",
      "441 0.0006720856763422489\n",
      "442 0.0006559730391018093\n",
      "443 0.0006401476566679776\n",
      "444 0.000624361855443567\n",
      "445 0.0006095350254327059\n",
      "446 0.0005964078009128571\n",
      "447 0.0005815986078232527\n",
      "448 0.000568479998037219\n",
      "449 0.0005552297225221992\n",
      "450 0.0005432729376479983\n",
      "451 0.0005311887362040579\n",
      "452 0.0005194214172661304\n",
      "453 0.000506376032717526\n",
      "454 0.0004947857232764363\n",
      "455 0.00048339171917177737\n",
      "456 0.00047304859617725015\n",
      "457 0.0004623420536518097\n",
      "458 0.00045252504060044885\n",
      "459 0.00044264379539527\n",
      "460 0.00043368953629396856\n",
      "461 0.00042429252061992884\n",
      "462 0.0004153662302996963\n",
      "463 0.0004074728931300342\n",
      "464 0.00039799808291718364\n",
      "465 0.00039025029400363564\n",
      "466 0.00038204307202249765\n",
      "467 0.0003744141722563654\n",
      "468 0.00036671420093625784\n",
      "469 0.00035962703987024724\n",
      "470 0.000352538307197392\n",
      "471 0.0003456943668425083\n",
      "472 0.0003388787154108286\n",
      "473 0.00033239932963624597\n",
      "474 0.0003252391761634499\n",
      "475 0.0003196854086127132\n",
      "476 0.0003133042191620916\n",
      "477 0.00030711351428180933\n",
      "478 0.0003011200751643628\n",
      "479 0.00029449223075062037\n",
      "480 0.00028914492577314377\n",
      "481 0.0002838574000634253\n",
      "482 0.00027892558136954904\n",
      "483 0.0002738715265877545\n",
      "484 0.00026816927129402757\n",
      "485 0.0002642058825585991\n",
      "486 0.0002591418451629579\n",
      "487 0.00025482726050540805\n",
      "488 0.0002505629672668874\n",
      "489 0.0002462536212988198\n",
      "490 0.00024118680448736995\n",
      "491 0.00023696315474808216\n",
      "492 0.00023299388703890145\n",
      "493 0.00022867137158755213\n",
      "494 0.00022481831547338516\n",
      "495 0.00022061461640987545\n",
      "496 0.0002168829960282892\n",
      "497 0.00021347963775042444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498 0.00020983633294235915\n",
      "499 0.00020643448806367815\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colour_dict import colour_dict\n",
    "from evaluation import name_to_rgb\n",
    "import numpy as np\n",
    "import random\n",
    "non_blue = []\n",
    "for colour, values in colour_dict.items():\n",
    "    if colour == 'blue':\n",
    "        blue = np.array(list((map(name_to_rgb, values))))\n",
    "    else:\n",
    "        non_blue.extend(list((map(name_to_rgb, values))))\n",
    "non_blue = np.array(non_blue)\n",
    "labels = [1]*len(blue) + [0]*len(non_blue)\n",
    "data = np.concatenate([blue, non_blue], axis=0)\n",
    "c = list(zip(data, labels))\n",
    "random.shuffle(c)\n",
    "data, labels = zip(*c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[l] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 1, 3, 10, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "x = torch.tensor(data, dtype=dtype)\n",
    "y = torch.tensor(labels, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 16.51317024230957\n",
      "1 16.430273056030273\n",
      "2 16.34917640686035\n",
      "3 16.269832611083984\n",
      "4 16.192203521728516\n",
      "5 16.116254806518555\n",
      "6 16.041940689086914\n",
      "7 15.969230651855469\n",
      "8 15.898088455200195\n",
      "9 15.82846736907959\n",
      "10 15.76034164428711\n",
      "11 15.693679809570312\n",
      "12 15.628440856933594\n",
      "13 15.564589500427246\n",
      "14 15.502103805541992\n",
      "15 15.44095230102539\n",
      "16 15.381096839904785\n",
      "17 15.322514533996582\n",
      "18 15.265174865722656\n",
      "19 15.209038734436035\n",
      "20 15.154094696044922\n",
      "21 15.100306510925293\n",
      "22 15.047647476196289\n",
      "23 14.996094703674316\n",
      "24 14.945624351501465\n",
      "25 14.89620590209961\n",
      "26 14.847823143005371\n",
      "27 14.800446510314941\n",
      "28 14.754055976867676\n",
      "29 14.708620071411133\n",
      "30 14.664132118225098\n",
      "31 14.620556831359863\n",
      "32 14.577885627746582\n",
      "33 14.536087036132812\n",
      "34 14.495145797729492\n",
      "35 14.455046653747559\n",
      "36 14.415766716003418\n",
      "37 14.377280235290527\n",
      "38 14.339578628540039\n",
      "39 14.302642822265625\n",
      "40 14.266448974609375\n",
      "41 14.230989456176758\n",
      "42 14.196239471435547\n",
      "43 14.162189483642578\n",
      "44 14.128816604614258\n",
      "45 14.096111297607422\n",
      "46 14.064053535461426\n",
      "47 14.032631874084473\n",
      "48 14.00182819366455\n",
      "49 13.971639633178711\n",
      "50 13.942034721374512\n",
      "51 13.913015365600586\n",
      "52 13.884559631347656\n",
      "53 13.85665512084961\n",
      "54 13.82929515838623\n",
      "55 13.802459716796875\n",
      "56 13.776144027709961\n",
      "57 13.75033187866211\n",
      "58 13.725010871887207\n",
      "59 13.700174331665039\n",
      "60 13.675811767578125\n",
      "61 13.651905059814453\n",
      "62 13.628448486328125\n",
      "63 13.605433464050293\n",
      "64 13.582847595214844\n",
      "65 13.56068229675293\n",
      "66 13.538926124572754\n",
      "67 13.517572402954102\n",
      "68 13.49660587310791\n",
      "69 13.476028442382812\n",
      "70 13.455821990966797\n",
      "71 13.435981750488281\n",
      "72 13.416500091552734\n",
      "73 13.397369384765625\n",
      "74 13.378579139709473\n",
      "75 13.360119819641113\n",
      "76 13.341991424560547\n",
      "77 13.324182510375977\n",
      "78 13.306681632995605\n",
      "79 13.289484977722168\n",
      "80 13.272591590881348\n",
      "81 13.25598430633545\n",
      "82 13.239664077758789\n",
      "83 13.223617553710938\n",
      "84 13.207847595214844\n",
      "85 13.192341804504395\n",
      "86 13.177094459533691\n",
      "87 13.162105560302734\n",
      "88 13.147360801696777\n",
      "89 13.132856369018555\n",
      "90 13.118593215942383\n",
      "91 13.104557991027832\n",
      "92 13.090751647949219\n",
      "93 13.077167510986328\n",
      "94 13.063798904418945\n",
      "95 13.050641059875488\n",
      "96 13.03769302368164\n",
      "97 13.024946212768555\n",
      "98 13.012394905090332\n",
      "99 12.999923706054688\n",
      "100 12.987442970275879\n",
      "101 12.975151062011719\n",
      "102 12.963045120239258\n",
      "103 12.951114654541016\n",
      "104 12.939361572265625\n",
      "105 12.92778205871582\n",
      "106 12.916370391845703\n",
      "107 12.905119895935059\n",
      "108 12.894031524658203\n",
      "109 12.88310432434082\n",
      "110 12.87232494354248\n",
      "111 12.861698150634766\n",
      "112 12.851215362548828\n",
      "113 12.840879440307617\n",
      "114 12.830681800842285\n",
      "115 12.8206205368042\n",
      "116 12.810694694519043\n",
      "117 12.800898551940918\n",
      "118 12.791231155395508\n",
      "119 12.78168773651123\n",
      "120 12.772269248962402\n",
      "121 12.762969017028809\n",
      "122 12.753785133361816\n",
      "123 12.744715690612793\n",
      "124 12.735761642456055\n",
      "125 12.726912498474121\n",
      "126 12.71817398071289\n",
      "127 12.7095365524292\n",
      "128 12.701001167297363\n",
      "129 12.692573547363281\n",
      "130 12.684236526489258\n",
      "131 12.675995826721191\n",
      "132 12.66784954071045\n",
      "133 12.659794807434082\n",
      "134 12.65183162689209\n",
      "135 12.643954277038574\n",
      "136 12.636161804199219\n",
      "137 12.628453254699707\n",
      "138 12.620828628540039\n",
      "139 12.613282203674316\n",
      "140 12.605814933776855\n",
      "141 12.59842300415039\n",
      "142 12.591108322143555\n",
      "143 12.583867073059082\n",
      "144 12.576695442199707\n",
      "145 12.56959342956543\n",
      "146 12.56256103515625\n",
      "147 12.555595397949219\n",
      "148 12.548701286315918\n",
      "149 12.541866302490234\n",
      "150 12.535093307495117\n",
      "151 12.528379440307617\n",
      "152 12.52173137664795\n",
      "153 12.515142440795898\n",
      "154 12.508607864379883\n",
      "155 12.502131462097168\n",
      "156 12.495706558227539\n",
      "157 12.489339828491211\n",
      "158 12.483026504516602\n",
      "159 12.476760864257812\n",
      "160 12.470550537109375\n",
      "161 12.46438217163086\n",
      "162 12.458271980285645\n",
      "163 12.452200889587402\n",
      "164 12.446176528930664\n",
      "165 12.440205574035645\n",
      "166 12.434273719787598\n",
      "167 12.428384780883789\n",
      "168 12.422541618347168\n",
      "169 12.41673755645752\n",
      "170 12.410971641540527\n",
      "171 12.405251502990723\n",
      "172 12.399568557739258\n",
      "173 12.393922805786133\n",
      "174 12.388315200805664\n",
      "175 12.382743835449219\n",
      "176 12.377208709716797\n",
      "177 12.371708869934082\n",
      "178 12.366241455078125\n",
      "179 12.360811233520508\n",
      "180 12.355412483215332\n",
      "181 12.350046157836914\n",
      "182 12.344710350036621\n",
      "183 12.33940601348877\n",
      "184 12.334135055541992\n",
      "185 12.328889846801758\n",
      "186 12.323674201965332\n",
      "187 12.318488121032715\n",
      "188 12.313331604003906\n",
      "189 12.308197975158691\n",
      "190 12.303095817565918\n",
      "191 12.298020362854004\n",
      "192 12.292970657348633\n",
      "193 12.287944793701172\n",
      "194 12.282944679260254\n",
      "195 12.27796745300293\n",
      "196 12.27301025390625\n",
      "197 12.268082618713379\n",
      "198 12.263176918029785\n",
      "199 12.258291244506836\n",
      "200 12.253430366516113\n",
      "201 12.248589515686035\n",
      "202 12.243770599365234\n",
      "203 12.238969802856445\n",
      "204 12.2341947555542\n",
      "205 12.229434967041016\n",
      "206 12.224695205688477\n",
      "207 12.219975471496582\n",
      "208 12.215276718139648\n",
      "209 12.210592269897461\n",
      "210 12.20592975616455\n",
      "211 12.201284408569336\n",
      "212 12.196651458740234\n",
      "213 12.192039489746094\n",
      "214 12.1874418258667\n",
      "215 12.18286418914795\n",
      "216 12.178300857543945\n",
      "217 12.173751831054688\n",
      "218 12.16922378540039\n",
      "219 12.164705276489258\n",
      "220 12.160202980041504\n",
      "221 12.155716896057129\n",
      "222 12.151243209838867\n",
      "223 12.146787643432617\n",
      "224 12.142342567443848\n",
      "225 12.13791275024414\n",
      "226 12.133493423461914\n",
      "227 12.129090309143066\n",
      "228 12.124700546264648\n",
      "229 12.12032413482666\n",
      "230 12.115958213806152\n",
      "231 12.111604690551758\n",
      "232 12.107263565063477\n",
      "233 12.102934837341309\n",
      "234 12.098616600036621\n",
      "235 12.094306945800781\n",
      "236 12.09001636505127\n",
      "237 12.085732460021973\n",
      "238 12.08145809173584\n",
      "239 12.077198028564453\n",
      "240 12.072944641113281\n",
      "241 12.068705558776855\n",
      "242 12.064473152160645\n",
      "243 12.060250282287598\n",
      "244 12.056039810180664\n",
      "245 12.051838874816895\n",
      "246 12.047646522521973\n",
      "247 12.043461799621582\n",
      "248 12.039288520812988\n",
      "249 12.035124778747559\n",
      "250 12.030970573425293\n",
      "251 12.026823043823242\n",
      "252 12.022686958312988\n",
      "253 12.018558502197266\n",
      "254 12.014434814453125\n",
      "255 12.010326385498047\n",
      "256 12.00622272491455\n",
      "257 12.002120971679688\n",
      "258 11.99803638458252\n",
      "259 11.993953704833984\n",
      "260 11.98988151550293\n",
      "261 11.985817909240723\n",
      "262 11.981758117675781\n",
      "263 11.977707862854004\n",
      "264 11.973666191101074\n",
      "265 11.969632148742676\n",
      "266 11.96560287475586\n",
      "267 11.96158218383789\n",
      "268 11.957566261291504\n",
      "269 11.953560829162598\n",
      "270 11.949559211730957\n",
      "271 11.945565223693848\n",
      "272 11.94157600402832\n",
      "273 11.937597274780273\n",
      "274 11.933621406555176\n",
      "275 11.929651260375977\n",
      "276 11.925689697265625\n",
      "277 11.921731948852539\n",
      "278 11.917783737182617\n",
      "279 11.913835525512695\n",
      "280 11.90989875793457\n",
      "281 11.905967712402344\n",
      "282 11.9020414352417\n",
      "283 11.898120880126953\n",
      "284 11.894205093383789\n",
      "285 11.89029312133789\n",
      "286 11.886388778686523\n",
      "287 11.882489204406738\n",
      "288 11.878596305847168\n",
      "289 11.874711036682129\n",
      "290 11.870826721191406\n",
      "291 11.866948127746582\n",
      "292 11.863078117370605\n",
      "293 11.859210014343262\n",
      "294 11.855344772338867\n",
      "295 11.85148811340332\n",
      "296 11.847636222839355\n",
      "297 11.843791961669922\n",
      "298 11.839947700500488\n",
      "299 11.836108207702637\n",
      "300 11.832276344299316\n",
      "301 11.828446388244629\n",
      "302 11.824625015258789\n",
      "303 11.820808410644531\n",
      "304 11.816990852355957\n",
      "305 11.813180923461914\n",
      "306 11.80937385559082\n",
      "307 11.805574417114258\n",
      "308 11.801780700683594\n",
      "309 11.797986030578613\n",
      "310 11.794198989868164\n",
      "311 11.79041576385498\n",
      "312 11.786636352539062\n",
      "313 11.782862663269043\n",
      "314 11.779088973999023\n",
      "315 11.775323867797852\n",
      "316 11.771560668945312\n",
      "317 11.767799377441406\n",
      "318 11.764046669006348\n",
      "319 11.760297775268555\n",
      "320 11.756551742553711\n",
      "321 11.75281047821045\n",
      "322 11.749070167541504\n",
      "323 11.74533462524414\n",
      "324 11.74160385131836\n",
      "325 11.737881660461426\n",
      "326 11.734156608581543\n",
      "327 11.730437278747559\n",
      "328 11.726722717285156\n",
      "329 11.723010063171387\n",
      "330 11.719303131103516\n",
      "331 11.715597152709961\n",
      "332 11.711896896362305\n",
      "333 11.708202362060547\n",
      "334 11.704507827758789\n",
      "335 11.700820922851562\n",
      "336 11.69713306427002\n",
      "337 11.693449020385742\n",
      "338 11.68977165222168\n",
      "339 11.686097145080566\n",
      "340 11.682426452636719\n",
      "341 11.678757667541504\n",
      "342 11.675090789794922\n",
      "343 11.671430587768555\n",
      "344 11.66777229309082\n",
      "345 11.664117813110352\n",
      "346 11.660467147827148\n",
      "347 11.656820297241211\n",
      "348 11.653176307678223\n",
      "349 11.649534225463867\n",
      "350 11.645896911621094\n",
      "351 11.642261505126953\n",
      "352 11.638629913330078\n",
      "353 11.635001182556152\n",
      "354 11.631376266479492\n",
      "355 11.627755165100098\n",
      "356 11.624137878417969\n",
      "357 11.620522499084473\n",
      "358 11.616912841796875\n",
      "359 11.613304138183594\n",
      "360 11.609696388244629\n",
      "361 11.606096267700195\n",
      "362 11.602499008178711\n",
      "363 11.598902702331543\n",
      "364 11.595308303833008\n",
      "365 11.591716766357422\n",
      "366 11.58813190460205\n",
      "367 11.584548950195312\n",
      "368 11.580967903137207\n",
      "369 11.577391624450684\n",
      "370 11.573819160461426\n",
      "371 11.570245742797852\n",
      "372 11.566678047180176\n",
      "373 11.563116073608398\n",
      "374 11.559553146362305\n",
      "375 11.55599308013916\n",
      "376 11.552436828613281\n",
      "377 11.548885345458984\n",
      "378 11.545333862304688\n",
      "379 11.541787147521973\n",
      "380 11.538241386413574\n",
      "381 11.534701347351074\n",
      "382 11.53116226196289\n",
      "383 11.527626037597656\n",
      "384 11.524093627929688\n",
      "385 11.520564079284668\n",
      "386 11.517037391662598\n",
      "387 11.51351261138916\n",
      "388 11.509989738464355\n",
      "389 11.506476402282715\n",
      "390 11.502959251403809\n",
      "391 11.499446868896484\n",
      "392 11.495936393737793\n",
      "393 11.492429733276367\n",
      "394 11.48892593383789\n",
      "395 11.485424041748047\n",
      "396 11.481927871704102\n",
      "397 11.478432655334473\n",
      "398 11.474939346313477\n",
      "399 11.47144889831543\n",
      "400 11.467962265014648\n",
      "401 11.4644775390625\n",
      "402 11.460993766784668\n",
      "403 11.45751667022705\n",
      "404 11.45404052734375\n",
      "405 11.450567245483398\n",
      "406 11.44709587097168\n",
      "407 11.443625450134277\n",
      "408 11.44015884399414\n",
      "409 11.436699867248535\n",
      "410 11.433241844177246\n",
      "411 11.429780960083008\n",
      "412 11.426331520080566\n",
      "413 11.422879219055176\n",
      "414 11.419428825378418\n",
      "415 11.415982246398926\n",
      "416 11.412540435791016\n",
      "417 11.409100532531738\n",
      "418 11.405659675598145\n",
      "419 11.402226448059082\n",
      "420 11.39879035949707\n",
      "421 11.395359992980957\n",
      "422 11.391932487487793\n",
      "423 11.388506889343262\n",
      "424 11.38508415222168\n",
      "425 11.381664276123047\n",
      "426 11.378247261047363\n",
      "427 11.374834060668945\n",
      "428 11.371420860290527\n",
      "429 11.368012428283691\n",
      "430 11.364603996276855\n",
      "431 11.361200332641602\n",
      "432 11.357799530029297\n",
      "433 11.354401588439941\n",
      "434 11.351004600524902\n",
      "435 11.347613334655762\n",
      "436 11.344220161437988\n",
      "437 11.340831756591797\n",
      "438 11.337443351745605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439 11.334063529968262\n",
      "440 11.330682754516602\n",
      "441 11.32730484008789\n",
      "442 11.323927879333496\n",
      "443 11.320552825927734\n",
      "444 11.317181587219238\n",
      "445 11.313814163208008\n",
      "446 11.310450553894043\n",
      "447 11.307084083557129\n",
      "448 11.30372428894043\n",
      "449 11.300366401672363\n",
      "450 11.297008514404297\n",
      "451 11.293659210205078\n",
      "452 11.290308952331543\n",
      "453 11.286958694458008\n",
      "454 11.283615112304688\n",
      "455 11.28027057647705\n",
      "456 11.276930809020996\n",
      "457 11.273590087890625\n",
      "458 11.270255088806152\n",
      "459 11.266923904418945\n",
      "460 11.263593673706055\n",
      "461 11.260265350341797\n",
      "462 11.256938934326172\n",
      "463 11.25361442565918\n",
      "464 11.250293731689453\n",
      "465 11.246975898742676\n",
      "466 11.243661880493164\n",
      "467 11.240348815917969\n",
      "468 11.237037658691406\n",
      "469 11.233728408813477\n",
      "470 11.230422019958496\n",
      "471 11.227119445800781\n",
      "472 11.223816871643066\n",
      "473 11.2205171585083\n",
      "474 11.217220306396484\n",
      "475 11.2139253616333\n",
      "476 11.2106351852417\n",
      "477 11.207345962524414\n",
      "478 11.204057693481445\n",
      "479 11.200774192810059\n",
      "480 11.197494506835938\n",
      "481 11.194214820861816\n",
      "482 11.190935134887695\n",
      "483 11.187661170959473\n",
      "484 11.184389114379883\n",
      "485 11.181119918823242\n",
      "486 11.177851676940918\n",
      "487 11.17458724975586\n",
      "488 11.171324729919434\n",
      "489 11.168066024780273\n",
      "490 11.16480541229248\n",
      "491 11.161550521850586\n",
      "492 11.158297538757324\n",
      "493 11.155047416687012\n",
      "494 11.151799201965332\n",
      "495 11.148550033569336\n",
      "496 11.145308494567871\n",
      "497 11.14206600189209\n",
      "498 11.138829231262207\n",
      "499 11.135591506958008\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "0 7.628746509552002\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "1 7.618115425109863\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "2 7.607580661773682\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "3 7.597138404846191\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "4 7.58678674697876\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "5 7.576523303985596\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "6 7.56634521484375\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "7 7.55625057220459\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "8 7.546236991882324\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "9 7.53630256652832\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "10 7.526444435119629\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "11 7.516661643981934\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "12 7.5069355964660645\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "13 7.497050762176514\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "14 7.487235069274902\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "15 7.477486610412598\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "16 7.467804431915283\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "17 7.458186149597168\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "18 7.448631286621094\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "19 7.439136981964111\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "20 7.4297027587890625\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "21 7.420327186584473\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "22 7.411007881164551\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "23 7.401744365692139\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "24 7.392535209655762\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "25 7.3833794593811035\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "26 7.374275207519531\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "27 7.3652215003967285\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "28 7.356217861175537\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "29 7.347262382507324\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "30 7.338354110717773\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "31 7.329492568969727\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "32 7.320676326751709\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "33 7.311904430389404\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "34 7.303175926208496\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "35 7.294489860534668\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "36 7.2858171463012695\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "37 7.277143955230713\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "38 7.268510341644287\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "39 7.259915828704834\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "40 7.251359462738037\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "41 7.242840766906738\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "42 7.234358787536621\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "43 7.225912570953369\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "44 7.217502117156982\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "45 7.2091264724731445\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "46 7.200784206390381\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "47 7.192476272583008\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "48 7.184200286865234\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "49 7.175957202911377\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "50 7.167745590209961\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "51 7.159565448760986\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "52 7.1514153480529785\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "53 7.143321990966797\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "54 7.135285377502441\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "55 7.127277851104736\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "56 7.119299411773682\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "57 7.111349105834961\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "58 7.103426933288574\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "59 7.095531940460205\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "60 7.087663650512695\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "61 7.079822540283203\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "62 7.072007656097412\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "63 7.064217567443848\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "64 7.056453227996826\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "65 7.0487141609191895\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "66 7.040999412536621\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "67 7.033308982849121\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "68 7.025642395019531\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "69 7.017999649047852\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "70 7.010379791259766\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "71 7.002782821655273\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "72 6.995208740234375\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "73 6.987656593322754\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "74 6.98012638092041\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "75 6.972618103027344\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "76 6.9651312828063965\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "77 6.957664966583252\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "78 6.950220108032227\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "79 6.942795753479004\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "80 6.935391426086426\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "81 6.92800760269165\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "82 6.920643329620361\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "83 6.913298606872559\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "84 6.905972957611084\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "85 6.898666858673096\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "86 6.8913798332214355\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "87 6.8841118812561035\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "88 6.876861572265625\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "89 6.869629859924316\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "90 6.8624162673950195\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "91 6.855220794677734\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "92 6.848042964935303\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "93 6.840882778167725\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "94 6.833739757537842\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "95 6.8266143798828125\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "96 6.81950569152832\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "97 6.812413692474365\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "98 6.805338382720947\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "99 6.798279762268066\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "100 6.7912373542785645\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "101 6.7842116355896\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "102 6.7772016525268555\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "103 6.77020788192749\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "104 6.7632293701171875\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "105 6.756267070770264\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "106 6.749320030212402\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "107 6.7423882484436035\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "108 6.735471725463867\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "109 6.728570938110352\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "110 6.721684455871582\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "111 6.714812755584717\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "112 6.707956314086914\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "113 6.701114177703857\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "114 6.694286823272705\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "115 6.687473773956299\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "116 6.680675029754639\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "117 6.673890590667725\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "118 6.667119979858398\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "119 6.660363674163818\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "120 6.653621196746826\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "121 6.646892070770264\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "122 6.640175819396973\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "123 6.633468151092529\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "124 6.626773357391357\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "125 6.620092868804932\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "126 6.613382816314697\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "127 6.6065144538879395\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "128 6.599652290344238\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "129 6.592493057250977\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "130 6.585348606109619\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "131 6.578219890594482\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "132 6.571106910705566\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "133 6.564008712768555\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "134 6.5569257736206055\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "135 6.5498576164245605\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "136 6.54280424118042\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "137 6.535766124725342\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "138 6.52874231338501\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "139 6.521733283996582\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "140 6.514739036560059\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "141 6.507758617401123\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "142 6.50079345703125\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "143 6.493841648101807\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "144 6.486905097961426\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "145 6.479982376098633\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "146 6.473073482513428\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "147 6.466178894042969\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "148 6.459298133850098\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "149 6.452430725097656\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "150 6.445577621459961\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "151 6.4387383460998535\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "152 6.431911945343018\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "153 6.425099849700928\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "154 6.418301105499268\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "155 6.411515712738037\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "156 6.404743671417236\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "157 6.397984504699707\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "158 6.391238689422607\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "159 6.384506702423096\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "160 6.377787113189697\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "161 6.3710808753967285\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "162 6.364387512207031\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "163 6.3577070236206055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "164 6.351039409637451\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "165 6.34438419342041\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "166 6.337742328643799\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "167 6.331112384796143\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "168 6.324495315551758\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "169 6.317890167236328\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "170 6.311221599578857\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "171 6.304564952850342\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "172 6.2979207038879395\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "173 6.29128885269165\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "174 6.284668922424316\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "175 6.278061389923096\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "176 6.271462440490723\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "177 6.264867305755615\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "178 6.258284091949463\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "179 6.251712799072266\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "180 6.245152950286865\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "181 6.23860502243042\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "182 6.23206901550293\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "183 6.225544452667236\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "184 6.21903133392334\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "185 6.212530136108398\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "186 6.206040382385254\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "187 6.199561595916748\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "188 6.193094730377197\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "189 6.186639308929443\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "190 6.180231094360352\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "191 6.173868656158447\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "192 6.16751766204834\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "193 6.161177635192871\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "194 6.154848098754883\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "195 6.148530006408691\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "196 6.1422224044799805\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "197 6.13592529296875\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "198 6.129639148712158\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "199 6.123363971710205\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "200 6.117098808288574\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "201 6.110844612121582\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "202 6.10460090637207\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "203 6.098364353179932\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "204 6.092046737670898\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "205 6.085740089416504\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "206 6.07944393157959\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "207 6.073157787322998\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "208 6.066882133483887\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "209 6.060616970062256\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "210 6.0543622970581055\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "211 6.048116683959961\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "212 6.041882514953613\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "213 6.03565788269043\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "214 6.029443264007568\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "215 6.023238658905029\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "216 6.017044544219971\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "217 6.010859966278076\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "218 6.004685878753662\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "219 5.998520851135254\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "220 5.992366313934326\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "221 5.986221790313721\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "222 5.980086803436279\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "223 5.973926067352295\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "224 5.967719554901123\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "225 5.961522579193115\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "226 5.95533561706543\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "227 5.949158191680908\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "228 5.942990303039551\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "229 5.936831951141357\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "230 5.930683612823486\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "231 5.924544334411621\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "232 5.91841459274292\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "233 5.912294387817383\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "234 5.90618371963501\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "235 5.900082588195801\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "236 5.893990516662598\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "237 5.8879075050354\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "238 5.881834506988525\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "239 5.875770568847656\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "240 5.869715213775635\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "241 5.8636698722839355\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "242 5.857633113861084\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "243 5.8516058921813965\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "244 5.845587730407715\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "245 5.839578151702881\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "246 5.833578109741211\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "247 5.827587127685547\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "248 5.821605205535889\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "249 5.815631866455078\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "250 5.809667587280273\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "251 5.803712368011475\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "252 5.797765731811523\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "253 5.791828155517578\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "254 5.7858991622924805\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "255 5.779979228973389\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "256 5.7740678787231445\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "257 5.768165588378906\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "258 5.762271881103516\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "259 5.756386756896973\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "260 5.750510215759277\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "261 5.74464225769043\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "262 5.73878288269043\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "263 5.732932090759277\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "264 5.727089881896973\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "265 5.721256256103516\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "266 5.715431213378906\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "267 5.709614276885986\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "268 5.703806400299072\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "269 5.6980061531066895\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "270 5.6922149658203125\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "271 5.686431884765625\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "272 5.680656909942627\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "273 5.674890518188477\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "274 5.669132232666016\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "275 5.663382053375244\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "276 5.65764045715332\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "277 5.651906967163086\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "278 5.646181583404541\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "279 5.640464782714844\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "280 5.634755611419678\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "281 5.629055023193359\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "282 5.623362064361572\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "283 5.617677211761475\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "284 5.612000465393066\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "285 5.606331825256348\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "286 5.60067081451416\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "287 5.59501838684082\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "288 5.589373588562012\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "289 5.583736896514893\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "290 5.578107833862305\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "291 5.572486877441406\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "292 5.566953659057617\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "293 5.561577320098877\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "294 5.556208610534668\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "295 5.55084753036499\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "296 5.545494079589844\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "297 5.54014778137207\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "298 5.53480863571167\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "299 5.529477596282959\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "300 5.524153232574463\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "301 5.518836498260498\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "302 5.513526916503906\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "303 5.508224964141846\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "304 5.502930164337158\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "305 5.497642517089844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "306 5.4923624992370605\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "307 5.48708963394165\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "308 5.481823444366455\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "309 5.476564884185791\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "310 5.4713134765625\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "311 5.46606969833374\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "312 5.460832595825195\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "313 5.455602169036865\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "314 5.450379371643066\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "315 5.445163726806641\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "316 5.43995475769043\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "317 5.434752941131592\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "318 5.429558753967285\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "319 5.424370765686035\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "320 5.419190406799316\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "321 5.4140167236328125\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "322 5.408849716186523\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "323 5.403690338134766\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "324 5.3985371589660645\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "325 5.393391132354736\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "326 5.388252258300781\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "327 5.383120059967041\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "328 5.377994537353516\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "329 5.3728766441345215\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "330 5.367764472961426\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "331 5.362659931182861\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "332 5.3575615882873535\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "333 5.352470397949219\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "334 5.347385883331299\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "335 5.342308044433594\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "336 5.337237358093262\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "337 5.332172870635986\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "338 5.327115058898926\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "339 5.322064399719238\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "340 5.317019939422607\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "341 5.311983108520508\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "342 5.306955814361572\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "343 5.301935195922852\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "344 5.296921730041504\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "345 5.291914939880371\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "346 5.286938190460205\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "347 5.281967639923096\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "348 5.277004241943359\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "349 5.2720465660095215\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "350 5.267095565795898\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "351 5.262150764465332\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "352 5.2572126388549805\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "353 5.2522807121276855\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "354 5.247354984283447\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "355 5.242435455322266\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "356 5.237522602081299\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "357 5.2326154708862305\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "358 5.227715015411377\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "359 5.22282075881958\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "360 5.21793270111084\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "361 5.2130513191223145\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "362 5.2081756591796875\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "363 5.203306198120117\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "364 5.1984429359436035\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "365 5.1935858726501465\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "366 5.188735485076904\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "367 5.1838908195495605\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "368 5.179052352905273\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "369 5.174219608306885\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "370 5.169393539428711\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "371 5.1645731925964355\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "372 5.159759044647217\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "373 5.154951095581055\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "374 5.150148868560791\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "375 5.145352840423584\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "376 5.140563011169434\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "377 5.13577938079834\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "378 5.1310014724731445\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "379 5.126229763031006\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "380 5.121464252471924\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "381 5.116705417633057\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "382 5.111966133117676\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "383 5.107233047485352\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "384 5.102505207061768\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "385 5.097783088684082\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "386 5.093067169189453\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "387 5.0883564949035645\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "388 5.083651542663574\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "389 5.078952789306641\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "390 5.074259281158447\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "391 5.069571495056152\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "392 5.064889430999756\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "393 5.060213565826416\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "394 5.055542945861816\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "395 5.050877571105957\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "396 5.046217918395996\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "397 5.041564464569092\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "398 5.036916255950928\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "399 5.032273769378662\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "400 5.02759313583374\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "401 5.022731781005859\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "402 5.017876625061035\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "403 5.013027667999268\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "404 5.008184432983398\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "405 5.003347396850586\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "406 4.998516082763672\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "407 4.993691444396973\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "408 4.988872051239014\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "409 4.984058856964111\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "410 4.979251861572266\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "411 4.97445011138916\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "412 4.9696550369262695\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "413 4.964870452880859\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "414 4.960108757019043\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "415 4.955352783203125\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "416 4.9506025314331055\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "417 4.945857524871826\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "418 4.941119194030762\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "419 4.936273574829102\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "420 4.931238174438477\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "421 4.926209449768066\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "422 4.921191692352295\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "423 4.916184902191162\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "424 4.911184787750244\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "425 4.906191349029541\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "426 4.9012041091918945\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "427 4.896223545074463\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "428 4.891249656677246\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "429 4.886281967163086\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "430 4.881320953369141\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "431 4.87636661529541\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "432 4.871418476104736\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "433 4.866477012634277\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "434 4.861541748046875\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "435 4.8566131591796875\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "436 4.851691246032715\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "437 4.846775531768799\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "438 4.841865539550781\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "439 4.836962699890137\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "440 4.832066059112549\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "441 4.827175617218018\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "442 4.822291851043701\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "443 4.817414283752441\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "444 4.8125433921813965\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "445 4.80767822265625\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "446 4.802820205688477\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "447 4.797967910766602\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "448 4.793121814727783\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "449 4.78828239440918\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "450 4.783449172973633\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "451 4.778622150421143\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "452 4.773804664611816\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "453 4.769014835357666\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "454 4.764246463775635\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "455 4.759483814239502\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "456 4.754726886749268\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "457 4.749976634979248\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "458 4.745232105255127\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "459 4.7404937744140625\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "460 4.735761642456055\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "461 4.731034755706787\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "462 4.726314544677734\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "463 4.72160005569458\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "464 4.716891765594482\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "465 4.712189197540283\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "466 4.707492828369141\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "467 4.7028021812438965\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "468 4.698117733001709\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "469 4.693439483642578\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "470 4.6887664794921875\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "471 4.6840996742248535\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "472 4.679439067840576\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "473 4.6747846603393555\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "474 4.670135498046875\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "475 4.665492534637451\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "476 4.660855293273926\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "477 4.656224250793457\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "478 4.651598930358887\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "479 4.646979331970215\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "480 4.6423659324646\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "481 4.637758731842041\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "482 4.633156776428223\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "483 4.628560543060303\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "484 4.623970985412598\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "485 4.619386672973633\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "486 4.614808082580566\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "487 4.610235691070557\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "488 4.605669021606445\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "489 4.601108074188232\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "490 4.596553325653076\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "491 4.59200382232666\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "492 4.587460517883301\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "493 4.58292293548584\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "494 4.578391075134277\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "495 4.5738654136657715\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "496 4.569345474243164\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "497 4.564830780029297\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "498 4.560322284698486\n",
      "torch.Size([42, 1])\n",
      "torch.Size([42, 1])\n",
      "499 4.555819511413574\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "    print(y_pred.shape)\n",
    "    print(y.shape)\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.937456130981445\n",
      "1 8.67393970489502\n",
      "2 8.438857078552246\n",
      "3 8.22853946685791\n",
      "4 8.040599822998047\n",
      "5 7.872669219970703\n",
      "6 7.722585201263428\n",
      "7 7.58842134475708\n",
      "8 7.468462944030762\n",
      "9 7.361173152923584\n",
      "10 7.265145301818848\n",
      "11 7.179040431976318\n",
      "12 7.101541996002197\n",
      "13 7.031330585479736\n",
      "14 6.967087268829346\n",
      "15 6.907540798187256\n",
      "16 6.851527690887451\n",
      "17 6.798018455505371\n",
      "18 6.746119976043701\n",
      "19 6.695060729980469\n",
      "20 6.644177436828613\n",
      "21 6.592900276184082\n",
      "22 6.540735244750977\n",
      "23 6.4872517585754395\n",
      "24 6.432066917419434\n",
      "25 6.374842643737793\n",
      "26 6.3152852058410645\n",
      "27 6.253146171569824\n",
      "28 6.188220500946045\n",
      "29 6.120344161987305\n",
      "30 6.0493903160095215\n",
      "31 5.975268840789795\n",
      "32 5.897928237915039\n",
      "33 5.817358016967773\n",
      "34 5.733591079711914\n",
      "35 5.646707057952881\n",
      "36 5.556832313537598\n",
      "37 5.464132785797119\n",
      "38 5.368805885314941\n",
      "39 5.271071434020996\n",
      "40 5.171156883239746\n",
      "41 5.069292068481445\n",
      "42 4.965702056884766\n",
      "43 4.8606085777282715\n",
      "44 4.754233360290527\n",
      "45 4.6468024253845215\n",
      "46 4.53855037689209\n",
      "47 4.4297261238098145\n",
      "48 4.320595741271973\n",
      "49 4.211439609527588\n",
      "50 4.102551460266113\n",
      "51 3.9942259788513184\n",
      "52 3.8867475986480713\n",
      "53 3.7803778648376465\n",
      "54 3.675344467163086\n",
      "55 3.5718352794647217\n",
      "56 3.4699997901916504\n",
      "57 3.3699569702148438\n",
      "58 3.2718048095703125\n",
      "59 3.175632953643799\n",
      "60 3.0815320014953613\n",
      "61 2.9895989894866943\n",
      "62 2.8999369144439697\n",
      "63 2.8126509189605713\n",
      "64 2.727839469909668\n",
      "65 2.64558744430542\n",
      "66 2.5659608840942383\n",
      "67 2.489004373550415\n",
      "68 2.4147443771362305\n",
      "69 2.3431904315948486\n",
      "70 2.2743406295776367\n",
      "71 2.2081825733184814\n",
      "72 2.14469313621521\n",
      "73 2.083836317062378\n",
      "74 2.0255608558654785\n",
      "75 1.969800591468811\n",
      "76 1.9164741039276123\n",
      "77 1.865490436553955\n",
      "78 1.8167519569396973\n",
      "79 1.7701599597930908\n",
      "80 1.725616455078125\n",
      "81 1.6830253601074219\n",
      "82 1.6422920227050781\n",
      "83 1.6033220291137695\n",
      "84 1.5660209655761719\n",
      "85 1.530294418334961\n",
      "86 1.49605131149292\n",
      "87 1.463204026222229\n",
      "88 1.431671380996704\n",
      "89 1.4013786315917969\n",
      "90 1.3722572326660156\n",
      "91 1.3442432880401611\n",
      "92 1.3172768354415894\n",
      "93 1.2913013696670532\n",
      "94 1.2662628889083862\n",
      "95 1.2421119213104248\n",
      "96 1.2188022136688232\n",
      "97 1.1962922811508179\n",
      "98 1.1745436191558838\n",
      "99 1.153520941734314\n",
      "100 1.1331902742385864\n",
      "101 1.1135199069976807\n",
      "102 1.0944788455963135\n",
      "103 1.076038122177124\n",
      "104 1.058169960975647\n",
      "105 1.040848970413208\n",
      "106 1.0240509510040283\n",
      "107 1.0077533721923828\n",
      "108 0.9919342994689941\n",
      "109 0.9765728116035461\n",
      "110 0.9616483449935913\n",
      "111 0.9471416473388672\n",
      "112 0.9330344796180725\n",
      "113 0.9193093776702881\n",
      "114 0.9059502482414246\n",
      "115 0.8929418325424194\n",
      "116 0.8802692890167236\n",
      "117 0.867918848991394\n",
      "118 0.8558771014213562\n",
      "119 0.844131588935852\n",
      "120 0.8326705694198608\n",
      "121 0.8214828968048096\n",
      "122 0.8105583190917969\n",
      "123 0.79988694190979\n",
      "124 0.7894595861434937\n",
      "125 0.7792671918869019\n",
      "126 0.7693014144897461\n",
      "127 0.7595542669296265\n",
      "128 0.7500181198120117\n",
      "129 0.7406858205795288\n",
      "130 0.7315507531166077\n",
      "131 0.7226063013076782\n",
      "132 0.7138461470603943\n",
      "133 0.7052643895149231\n",
      "134 0.6968555450439453\n",
      "135 0.6886135935783386\n",
      "136 0.680533766746521\n",
      "137 0.6726111173629761\n",
      "138 0.664840579032898\n",
      "139 0.6572178602218628\n",
      "140 0.6497383117675781\n",
      "141 0.6423978209495544\n",
      "142 0.6351920962333679\n",
      "143 0.6281176805496216\n",
      "144 0.6211704611778259\n",
      "145 0.6143469214439392\n",
      "146 0.6076438426971436\n",
      "147 0.6010577082633972\n",
      "148 0.5945852994918823\n",
      "149 0.5882236361503601\n",
      "150 0.5819697976112366\n",
      "151 0.5758209824562073\n",
      "152 0.569774329662323\n",
      "153 0.563827395439148\n",
      "154 0.557977557182312\n",
      "155 0.5522224307060242\n",
      "156 0.5465596914291382\n",
      "157 0.5409870147705078\n",
      "158 0.5355021357536316\n",
      "159 0.5301030874252319\n",
      "160 0.5247877836227417\n",
      "161 0.5195541381835938\n",
      "162 0.5144005417823792\n",
      "163 0.5093247890472412\n",
      "164 0.5043253302574158\n",
      "165 0.4994003176689148\n",
      "166 0.49454808235168457\n",
      "167 0.48976704478263855\n",
      "168 0.485055536031723\n",
      "169 0.4804120361804962\n",
      "170 0.47583508491516113\n",
      "171 0.47132325172424316\n",
      "172 0.4668750762939453\n",
      "173 0.4624893367290497\n",
      "174 0.4581645131111145\n",
      "175 0.45389947295188904\n",
      "176 0.44969281554222107\n",
      "177 0.4455435276031494\n",
      "178 0.44145023822784424\n",
      "179 0.437411904335022\n",
      "180 0.4334273934364319\n",
      "181 0.42949575185775757\n",
      "182 0.42561572790145874\n",
      "183 0.4217863380908966\n",
      "184 0.41800668835639954\n",
      "185 0.4142756760120392\n",
      "186 0.41059255599975586\n",
      "187 0.4069562256336212\n",
      "188 0.40336576104164124\n",
      "189 0.3998205065727234\n",
      "190 0.3963194191455841\n",
      "191 0.3928617835044861\n",
      "192 0.3894467353820801\n",
      "193 0.386073499917984\n",
      "194 0.3827413320541382\n",
      "195 0.3794495463371277\n",
      "196 0.37619733810424805\n",
      "197 0.3729839622974396\n",
      "198 0.3698088824748993\n",
      "199 0.36667120456695557\n",
      "200 0.36357051134109497\n",
      "201 0.3605059087276459\n",
      "202 0.3574770987033844\n",
      "203 0.35448315739631653\n",
      "204 0.35152360796928406\n",
      "205 0.3485979735851288\n",
      "206 0.345705509185791\n",
      "207 0.34284573793411255\n",
      "208 0.34001821279525757\n",
      "209 0.33722224831581116\n",
      "210 0.3344574272632599\n",
      "211 0.33172330260276794\n",
      "212 0.3290192484855652\n",
      "213 0.3263448178768158\n",
      "214 0.32369959354400635\n",
      "215 0.32108306884765625\n",
      "216 0.31849485635757446\n",
      "217 0.31593450903892517\n",
      "218 0.31340155005455017\n",
      "219 0.31089556217193604\n",
      "220 0.3084162175655365\n",
      "221 0.3059629797935486\n",
      "222 0.3035355806350708\n",
      "223 0.3011336326599121\n",
      "224 0.2987566590309143\n",
      "225 0.2964043617248535\n",
      "226 0.29407644271850586\n",
      "227 0.29177242517471313\n",
      "228 0.2894919812679291\n",
      "229 0.287234902381897\n",
      "230 0.2850007116794586\n",
      "231 0.2827892303466797\n",
      "232 0.2805999517440796\n",
      "233 0.27843278646469116\n",
      "234 0.2762872576713562\n",
      "235 0.2741631269454956\n",
      "236 0.2720601260662079\n",
      "237 0.26997795701026917\n",
      "238 0.26791632175445557\n",
      "239 0.265874981880188\n",
      "240 0.26385366916656494\n",
      "241 0.26185208559036255\n",
      "242 0.2598699927330017\n",
      "243 0.25790706276893616\n",
      "244 0.25596314668655396\n",
      "245 0.254038006067276\n",
      "246 0.25213128328323364\n",
      "247 0.2502429187297821\n",
      "248 0.24837249517440796\n",
      "249 0.24651989340782166\n",
      "250 0.24468494951725006\n",
      "251 0.24286727607250214\n",
      "252 0.2410667985677719\n",
      "253 0.2392832636833191\n",
      "254 0.23751643300056458\n",
      "255 0.23576611280441284\n",
      "256 0.23403216898441315\n",
      "257 0.23231437802314758\n",
      "258 0.23061248660087585\n",
      "259 0.22892636060714722\n",
      "260 0.22725577652454376\n",
      "261 0.22560064494609833\n",
      "262 0.22396066784858704\n",
      "263 0.22233568131923676\n",
      "264 0.2207256257534027\n",
      "265 0.21913020312786102\n",
      "266 0.21754929423332214\n",
      "267 0.21598272025585175\n",
      "268 0.21443037688732147\n",
      "269 0.21289198100566864\n",
      "270 0.21136748790740967\n",
      "271 0.20985674858093262\n",
      "272 0.208359494805336\n",
      "273 0.2068757265806198\n",
      "274 0.20540522038936615\n",
      "275 0.2039477676153183\n",
      "276 0.2025032937526703\n",
      "277 0.20107172429561615\n",
      "278 0.19965273141860962\n",
      "279 0.1982463151216507\n",
      "280 0.19685232639312744\n",
      "281 0.1954706758260727\n",
      "282 0.19410109519958496\n",
      "283 0.19274359941482544\n",
      "284 0.19139796495437622\n",
      "285 0.19006408751010895\n",
      "286 0.18874189257621765\n",
      "287 0.18743117153644562\n",
      "288 0.18613183498382568\n",
      "289 0.18484385311603546\n",
      "290 0.18356697261333466\n",
      "291 0.18230114877223969\n",
      "292 0.1810462474822998\n",
      "293 0.17980223894119263\n",
      "294 0.17856886982917786\n",
      "295 0.1773461401462555\n",
      "296 0.1761338710784912\n",
      "297 0.17493203282356262\n",
      "298 0.1737404316663742\n",
      "299 0.17255903780460358\n",
      "300 0.1713877022266388\n",
      "301 0.1702263206243515\n",
      "302 0.16907478868961334\n",
      "303 0.1679331213235855\n",
      "304 0.1668010652065277\n",
      "305 0.16567859053611755\n",
      "306 0.16456562280654907\n",
      "307 0.16346204280853271\n",
      "308 0.16236774623394012\n",
      "309 0.16128262877464294\n",
      "310 0.1602066457271576\n",
      "311 0.1591397374868393\n",
      "312 0.15808168053627014\n",
      "313 0.1570325493812561\n",
      "314 0.15599216520786285\n",
      "315 0.15496043860912323\n",
      "316 0.15393730998039246\n",
      "317 0.15292270481586456\n",
      "318 0.1519165188074112\n",
      "319 0.15091866254806519\n",
      "320 0.14992915093898773\n",
      "321 0.14894771575927734\n",
      "322 0.14797446131706238\n",
      "323 0.1470092236995697\n",
      "324 0.14605197310447693\n",
      "325 0.14510254561901093\n",
      "326 0.1441609412431717\n",
      "327 0.14322705566883087\n",
      "328 0.14230087399482727\n",
      "329 0.14138224720954895\n",
      "330 0.14047111570835114\n",
      "331 0.13956741988658905\n",
      "332 0.1386711150407791\n",
      "333 0.13778209686279297\n",
      "334 0.13690032064914703\n",
      "335 0.13602574169635773\n",
      "336 0.1351582407951355\n",
      "337 0.13429774343967438\n",
      "338 0.13344421982765198\n",
      "339 0.13259762525558472\n",
      "340 0.13175779581069946\n",
      "341 0.13092482089996338\n",
      "342 0.13009856641292572\n",
      "343 0.12927892804145813\n",
      "344 0.12846584618091583\n",
      "345 0.12765930593013763\n",
      "346 0.12685921788215637\n",
      "347 0.12606553733348846\n",
      "348 0.12527821958065033\n",
      "349 0.12449721992015839\n",
      "350 0.12372240424156189\n",
      "351 0.12295375019311905\n",
      "352 0.12219125032424927\n",
      "353 0.12143483757972717\n",
      "354 0.12068432569503784\n",
      "355 0.11993984133005142\n",
      "356 0.11920124292373657\n",
      "357 0.11846847832202911\n",
      "358 0.11774148792028427\n",
      "359 0.11702029407024384\n",
      "360 0.11630469560623169\n",
      "361 0.1155947744846344\n",
      "362 0.11489041149616241\n",
      "363 0.11419162154197693\n",
      "364 0.11349829286336899\n",
      "365 0.11281033605337143\n",
      "366 0.11212781071662903\n",
      "367 0.11145063489675522\n",
      "368 0.11077871918678284\n",
      "369 0.11011204868555069\n",
      "370 0.1094505712389946\n",
      "371 0.108794204890728\n",
      "372 0.10814294964075089\n",
      "373 0.10749676078557968\n",
      "374 0.10685562342405319\n",
      "375 0.1062193512916565\n",
      "376 0.10558800399303436\n",
      "377 0.10496155917644501\n",
      "378 0.10433998703956604\n",
      "379 0.10372313857078552\n",
      "380 0.10311108827590942\n",
      "381 0.10250367969274521\n",
      "382 0.10190095007419586\n",
      "383 0.10130282491445541\n",
      "384 0.10070928186178207\n",
      "385 0.10012028366327286\n",
      "386 0.09953580796718597\n",
      "387 0.09895576536655426\n",
      "388 0.09838012605905533\n",
      "389 0.09780887514352798\n",
      "390 0.09724193066358566\n",
      "391 0.09667930752038956\n",
      "392 0.09612095355987549\n",
      "393 0.09556683152914047\n",
      "394 0.09501689672470093\n",
      "395 0.09447108954191208\n",
      "396 0.09392941743135452\n",
      "397 0.09339180588722229\n",
      "398 0.09285823255777359\n",
      "399 0.09232869744300842\n",
      "400 0.09180308133363724\n",
      "401 0.091281458735466\n",
      "402 0.09076368808746338\n",
      "403 0.09024977684020996\n",
      "404 0.08973976969718933\n",
      "405 0.08923350274562836\n",
      "406 0.08873102068901062\n",
      "407 0.08823225647211075\n",
      "408 0.08773721009492874\n",
      "409 0.08724582940340042\n",
      "410 0.0867580696940422\n",
      "411 0.08627399057149887\n",
      "412 0.0857933759689331\n",
      "413 0.08531638234853745\n",
      "414 0.08484288305044174\n",
      "415 0.084372878074646\n",
      "416 0.08390631526708603\n",
      "417 0.08344316482543945\n",
      "418 0.08298344165086746\n",
      "419 0.0825270339846611\n",
      "420 0.08207397907972336\n",
      "421 0.08162419497966766\n",
      "422 0.08117774873971939\n",
      "423 0.0807344987988472\n",
      "424 0.08029451221227646\n",
      "425 0.0798577144742012\n",
      "426 0.07942406088113785\n",
      "427 0.0789935514330864\n",
      "428 0.07856619358062744\n",
      "429 0.07814183831214905\n",
      "430 0.07772066444158554\n",
      "431 0.07730238884687424\n",
      "432 0.07688721269369125\n",
      "433 0.07647499442100525\n",
      "434 0.07606571167707443\n",
      "435 0.0756593570113182\n",
      "436 0.07525594532489777\n",
      "437 0.07485540211200714\n",
      "438 0.07445771247148514\n",
      "439 0.07406286150217056\n",
      "440 0.07367081940174103\n",
      "441 0.07328153401613235\n",
      "442 0.07289502024650574\n",
      "443 0.07251130044460297\n",
      "444 0.0721302255988121\n",
      "445 0.07175188511610031\n",
      "446 0.07137621194124222\n",
      "447 0.07100315392017365\n",
      "448 0.07063276320695877\n",
      "449 0.07026490569114685\n",
      "450 0.06989969313144684\n",
      "451 0.06953700631856918\n",
      "452 0.06917687505483627\n",
      "453 0.06881926208734512\n",
      "454 0.06846413016319275\n",
      "455 0.06811146438121796\n",
      "456 0.06776127964258194\n",
      "457 0.06741355359554291\n",
      "458 0.0670681744813919\n",
      "459 0.0667252391576767\n",
      "460 0.06638465076684952\n",
      "461 0.06604640185832977\n",
      "462 0.06571051478385925\n",
      "463 0.06537691503763199\n",
      "464 0.06504564732313156\n",
      "465 0.064716637134552\n",
      "466 0.06438988447189331\n",
      "467 0.06406538933515549\n",
      "468 0.06374311447143555\n",
      "469 0.06342298537492752\n",
      "470 0.06310509890317917\n",
      "471 0.06278935819864273\n",
      "472 0.062475766986608505\n",
      "473 0.062164317816495895\n",
      "474 0.06185495853424072\n",
      "475 0.061547741293907166\n",
      "476 0.06124252453446388\n",
      "477 0.06093945726752281\n",
      "478 0.06063837930560112\n",
      "479 0.060339316725730896\n",
      "480 0.06004229187965393\n",
      "481 0.05974728986620903\n",
      "482 0.059454239904880524\n",
      "483 0.059163160622119904\n",
      "484 0.05887400358915329\n",
      "485 0.05858679488301277\n",
      "486 0.05830151587724686\n",
      "487 0.05801811069250107\n",
      "488 0.057736605405807495\n",
      "489 0.05745697394013405\n",
      "490 0.05717916041612625\n",
      "491 0.05690320208668709\n",
      "492 0.05662911757826805\n",
      "493 0.05635679513216019\n",
      "494 0.05608626827597618\n",
      "495 0.05581754073500633\n",
      "496 0.05555058643221855\n",
      "497 0.05528537184000015\n",
      "498 0.05502186715602875\n",
      "499 0.05476013198494911\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8366e-06], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.4496e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([6.9120e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0004], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0000], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([3.8694e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([3.3834e-07], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.9110], grad_fn=<SelectBackward>) tensor([1.])\n",
      "tensor([0.0055], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([3.2951e-07], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0960], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0890], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.9950], grad_fn=<SelectBackward>) tensor([1.])\n",
      "tensor([4.7561e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([1.6478e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([9.7925e-07], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([1.0000], grad_fn=<SelectBackward>) tensor([1.])\n",
      "tensor([0.9999], grad_fn=<SelectBackward>) tensor([1.])\n",
      "tensor([0.9999], grad_fn=<SelectBackward>) tensor([1.])\n",
      "tensor([0.8582], grad_fn=<SelectBackward>) tensor([1.])\n",
      "tensor([4.7001e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([1.7390e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([3.0817e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0000], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([4.5494e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0098], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0000], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([4.5601e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([2.2497e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0137], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([2.8675e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0902], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0319], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.9934], grad_fn=<SelectBackward>) tensor([1.])\n",
      "tensor([0.0002], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0006], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([3.7149e-07], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([3.6600e-06], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0002], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.0050], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([0.9973], grad_fn=<SelectBackward>) tensor([1.])\n",
      "tensor([3.4183e-07], grad_fn=<SelectBackward>) tensor([0.])\n",
      "tensor([1.0000], grad_fn=<SelectBackward>) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "for pred, true in zip(y_pred, y):\n",
    "    print(pred, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prob_model import NeuralColourModel\n",
    "from evaluation import test_colour_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = list(zip(data,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_data():\n",
    "    return random.choice(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx, w = draw_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = NeuralColourModel('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1924, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3808, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3962, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3338, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2341, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3317, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3282, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3927, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3316, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3160, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3179, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3540, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3510, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3544, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3352, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4098, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3488, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3543, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3158, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2457, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2142, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3894, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3510, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2343, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3203, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3397, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2247, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3959, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3236, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3591, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4244, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3897, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3938, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1954, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1864, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3367, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3440, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3156, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3896, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3439, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2144, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.2144, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4243, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3538, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3312, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3501, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4062, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3922, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3485, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3936, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3500, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3311, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3484, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2346, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3347, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2141, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3921, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3364, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3360, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1866, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3536, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3504, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2146, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1929, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4092, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3536, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3392, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1930, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2250, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3275, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3359, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3954, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3172, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3152, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3535, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3537, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3888, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3330, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3932, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3534, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2148, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.2122, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2461, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3274, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3891, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2348, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3536, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3918, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2462, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2122, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3504, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.4089, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3230, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3496, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2348, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3196, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3480, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3890, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2148, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3583, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3532, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3502, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3480, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3356, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4235, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3359, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3432, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3949, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3194, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2145, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4054, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3148, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1960, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2145, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3915, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2124, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1959, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3387, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3271, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3305, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3227, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.1870, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2125, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3357, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3478, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3226, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3530, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2151, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2151, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3341, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3237, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3914, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2255, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3341, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3492, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2255, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1871, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3304, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3492, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2465, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3226, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.2465, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3192, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3429, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3269, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3303, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4232, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1934, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3530, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3491, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3268, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3302, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3339, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3490, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3475, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2256, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3224, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2127, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3351, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3884, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3528, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3301, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2127, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2354, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3144, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3164, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3522, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2153, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3143, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1936, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3910, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3322, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3496, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3425, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3496, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3163, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3221, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3265, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2155, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3788, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3380, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3349, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3264, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2129, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3472, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1874, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2259, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3141, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4078, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3187, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3231, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.4078, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3880, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3219, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3524, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3524, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3879, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3470, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3318, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3229, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3317, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3376, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2472, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3159, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3906, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3376, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3490, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2154, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3159, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3420, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3874, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.1940, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3483, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1967, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1877, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3520, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2158, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3217, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3420, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1967, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3491, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.1877, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3938, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3904, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1967, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3515, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3904, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3157, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3873, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3520, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3903, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3373, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3418, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3257, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1879, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3487, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3343, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3292, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2156, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3257, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3257, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3780, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2161, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3935, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3874, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3256, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2264, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3901, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3518, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2264, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3479, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3901, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1969, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3341, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3180, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3934, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3222, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1943, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3370, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3485, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3516, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.4069, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3564, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3932, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3563, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4217, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3563, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3211, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1971, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2478, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3339, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.2136, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3515, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4035, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4216, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3252, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2267, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3898, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3513, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1882, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3338, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.2267, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3514, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2267, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3210, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3475, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3288, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3475, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3775, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3460, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3906, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3218, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3511, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2268, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2480, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3512, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3512, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3512, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3480, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4063, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3480, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.2139, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1946, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2480, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2138, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2366, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3150, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3251, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3895, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3511, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3559, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3481, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3509, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1947, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1884, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1884, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2166, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3364, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1973, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3773, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3216, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3481, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3927, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3322, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1947, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3334, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4029, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3306, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3557, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4211, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2270, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3362, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4061, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3864, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3248, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3172, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3146, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1948, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3455, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1975, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2164, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3124, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3320, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2164, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3556, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2271, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3478, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3863, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3213, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2369, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3405, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3171, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3924, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3404, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3860, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4208, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3862, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3281, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2370, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3211, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3476, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3403, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2170, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1887, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3475, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3302, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3505, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3358, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3897, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3245, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2371, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3860, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3328, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3504, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3120, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3451, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3765, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3243, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3474, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3765, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3243, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3550, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3355, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3550, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3325, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3206, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3325, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3327, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4203, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.2374, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2173, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3325, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1953, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3501, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3763, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3494, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3500, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3471, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2170, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3297, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3352, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3164, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3884, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3313, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2174, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3469, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1954, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3884, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3884, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3461, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2147, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1954, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3138, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3498, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3322, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3312, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3854, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2278, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3137, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3195, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2176, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3498, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3273, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3162, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3467, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3459, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3349, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3310, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3444, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3323, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1893, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3490, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3495, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3193, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3161, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2493, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3850, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3880, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4014, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3880, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1984, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3851, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3111, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2150, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2493, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3879, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3886, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4196, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3159, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3270, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3291, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3910, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3441, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3465, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3463, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3849, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2380, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1896, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3755, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3493, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3754, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3540, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3131, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3539, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1896, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3197, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3491, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3196, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1959, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3306, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3485, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3196, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3876, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4192, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3266, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3227, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3460, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3226, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3537, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3316, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3451, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3105, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2284, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3303, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3225, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3264, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3482, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3487, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4006, grad_fn=<MseLossBackward>)\n",
      "{'tp': 9, 'fp': 33, 'fn': 0, 'tn': 0}\n",
      "tensor(0.3481, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3339, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3487, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3749, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3480, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3285, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3227, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2185, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3314, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    fx, w = draw_data()\n",
    "    cm.update(fx, w)\n",
    "    if i % 10 == 0:\n",
    "        results = test_colour_model(cm)\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
